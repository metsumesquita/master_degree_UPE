{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/metsumesquita/master_degree_UPE/blob/main/artigo/Hybrid_Model_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Escrever o artigo no overleaf ⁉[overleaf_artigo](https://pt.overleaf.com/9677853253txwnpcyrdcmg#ce7095)"
      ],
      "metadata": {
        "id": "V97sE8lcOW9i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Dataset que jáe estão certos\n",
        "#-Exchange rate\n",
        "#-The sunspot (o nosso vai ate 1988)\n",
        "#the Canadian lynx\n",
        "\n",
        "---\n",
        "\n",
        "#dataset que foram usados\n",
        "\n",
        "#Exchange rate: foi passada pelo próprio khashei (o autor do artigo que estás usando como base) para a gente.\n",
        "# The exchange rate data set used in this paper contains weekly observations from 1980 to 1993\n",
        "#https://github.com/metsumesquita/master_degree_UPE/blob/main/Datasets/US-UK.csv\n",
        "\n",
        "#-The sunspot data considered in this paper include the annual number of sunspot from 1700 to 1987()\n",
        "\n",
        "#the Canadian lynx annual record of the number of Canadian lynx from 1821 to 1934\n",
        "#https://github.com/metsumesquita/master_degree_UPE/blob/main/Datasets/lynx.csv\n",
        "\n",
        "#Falta ??\n",
        "\n",
        "#nao sei se os dados entao todo certos\n",
        "#The closing Nikkei 225 index (N225) data set covers daily stock prices from 2006/03/03 to 2010/04/01\n",
        "\n",
        "#Unico artigo que falta\n",
        "#- wind speed data in Colorado State are used for simulating series hybrid models.\n",
        "\"\"\"\n",
        "---\n",
        "mas como a maioria desses dataset são amplamente usamos normalmente nao precisam de grande tratamento , colocaremos a normalização entre 1 e 0,\n",
        "o janelamento pode receber um melhoramento escolhando a melhor lag\n",
        "deve ser feita o salvamento de cada previsao que for feita e tambem de cada execução\n",
        "deve ser feita uma comparação entre o modelo antes do algoritmo de melhorando para os parametros e depois da utilização de algoritmos geneticos\n",
        "\n",
        "deve focar primeiramente na criaçao dos modelos por ser algo mais trabalhoso e demorado do que o resto do projeto\n",
        "\n",
        "alem disso existem muitas formas de realizar a combinçao de modelos como por exemplo o modelo de ponderaçao\n",
        "o modelo de funçao o modelo de combinaçao utilizando knn,e alem disso temos formas de selçao como a seleçao dinmaica (DYN), a do oraculo ou oracle\n",
        "e tambem o do pior modelo\n",
        "\n",
        "- importante criar omodelo primeiro\n",
        "ARIMA–MLP\n",
        "ARIMA-SVM\n",
        "MLP–ARIMA\n",
        "SVM-ARIMA\n",
        "\n",
        "\n",
        "ao utilizamos as metricas MAE,MSE ou RMAE , deve ser explicado o uso / defender ele e dar algum tipo de base (ref de artigo paper)\n",
        "\n",
        "tambem podemos utilizar o github dispobinizado pelo professor domingos\n",
        "\n",
        "\n",
        "https://github.com/domingos108/hybrid_system_forecast/blob/master/additive_hybrid_model_mlp.ipynb\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "zqB4F_mm5rYd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#instalações e importações"
      ],
      "metadata": {
        "id": "Kc_oFhz6Let0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install sktime"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NG2qV0VdKSB4",
        "outputId": "b13b6577-00fd-4899-e285-28d6639e5940"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sktime\n",
            "  Downloading sktime-0.37.0-py3-none-any.whl.metadata (34 kB)\n",
            "Requirement already satisfied: joblib<1.5,>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from sktime) (1.4.2)\n",
            "Requirement already satisfied: numpy<2.3,>=1.21 in /usr/local/lib/python3.11/dist-packages (from sktime) (2.2.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from sktime) (25.0)\n",
            "Requirement already satisfied: pandas<2.3.0,>=1.1 in /usr/local/lib/python3.11/dist-packages (from sktime) (2.2.3)\n",
            "Collecting scikit-base<0.13.0,>=0.6.1 (from sktime)\n",
            "  Downloading scikit_base-0.12.2-py3-none-any.whl.metadata (8.8 kB)\n",
            "Requirement already satisfied: scikit-learn<1.7.0,>=0.24 in /usr/local/lib/python3.11/dist-packages (from sktime) (1.6.1)\n",
            "Requirement already satisfied: scipy<2.0.0,>=1.2 in /usr/local/lib/python3.11/dist-packages (from sktime) (1.15.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<2.3.0,>=1.1->sktime) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<2.3.0,>=1.1->sktime) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<2.3.0,>=1.1->sktime) (2025.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn<1.7.0,>=0.24->sktime) (3.6.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<2.3.0,>=1.1->sktime) (1.17.0)\n",
            "Downloading sktime-0.37.0-py3-none-any.whl (37.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m37.0/37.0 MB\u001b[0m \u001b[31m50.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scikit_base-0.12.2-py3-none-any.whl (142 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m142.7/142.7 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: scikit-base, sktime\n",
            "Successfully installed scikit-base-0.12.2 sktime-0.37.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import keras as ks\n",
        "import tensorflow as tf\n"
      ],
      "metadata": {
        "id": "OQOHq-Is53Co"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import LSTM\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error"
      ],
      "metadata": {
        "id": "pwS8NahieI4o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from pandas.plotting import autocorrelation_plot\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "from statsmodels.tsa.stattools import adfuller\n",
        "from sklearn.metrics import mean_squared_error"
      ],
      "metadata": {
        "id": "KUOh3SyvEDXF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#carregamento dos conjuntos"
      ],
      "metadata": {
        "id": "mklwaIXWqacD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sktime.datasets import load_lynx\n",
        "lynx_data = load_lynx()"
      ],
      "metadata": {
        "id": "eFNpNDs2KNU9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#lynx\n",
        "####the Canadian lynx annual record of the number of Canadian lynx from 1821 to 1934"
      ],
      "metadata": {
        "id": "FBzV_jg6pCKI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#lynx\n",
        "lynx_data.head()\n",
        "lynx_data.tail()\n"
      ],
      "metadata": {
        "id": "dQRSY0oJXJjI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lynx_data.info()"
      ],
      "metadata": {
        "id": "8plz6Gd0so95"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(lynx_data.index)\n",
        "print(type(lynx_data.index))"
      ],
      "metadata": {
        "id": "7VUYeRrAo7lL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lynx_data.index = lynx_data.index.to_timestamp()"
      ],
      "metadata": {
        "id": "UkMVAOSUpuHT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "plt.figure(figsize=(18, 4))\n",
        "\n",
        "plt.plot(dk1.index, dk1['PM10'], label='PM10')\n",
        "y.plot(title=\"Lynx Trappings\")\n",
        "plt.xlabel(\"Year\")\n",
        "plt.ylabel(\"Number of Trappings\")\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "I4yUNelUqe3J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#sunpot\n",
        "####The sunspot data considered in this paper include the annual number ofsunspot from 1700 to 1987"
      ],
      "metadata": {
        "id": "HM5vVi_Io_DX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#sunpot\n",
        "sunpot_data.head()\n",
        "sunpot_data.tail()\n"
      ],
      "metadata": {
        "id": "Xoz6ZOlFoox6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "sunpot_data.info()\n"
      ],
      "metadata": {
        "id": "cyUWsnTCsuwR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(sunpot_data.index)\n",
        "print(type(sunpot_data.index))"
      ],
      "metadata": {
        "id": "GUbUdzdJpGa1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sunpot_data.index = sunpot_data.index.to_timestamp()"
      ],
      "metadata": {
        "id": "nnD4E0VApu3p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(18, 4))\n",
        "plt.plot(dk1.index, dk1['PM10'], label='PM10')\n",
        "y.plot(title=\"sunport of light per year\")\n",
        "plt.xlabel(\"Year\")\n",
        "plt.ylabel(\"Number of sunport of light\")\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "kUufPIAiqgP3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#exchange rate\n",
        "####The exchange rate data set used in this paper contains weekly observations from 1980 to 1993"
      ],
      "metadata": {
        "id": "Fyz26vSDpEYg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#exchange\n",
        "exchange_data.head()\n",
        "exchange_data.tail()"
      ],
      "metadata": {
        "id": "Zb4eDp0voqXQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "exchange_data.info()"
      ],
      "metadata": {
        "id": "oG4y8BwBsxSN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(exchange_data.index)\n",
        "print(type(exchange_data.index))"
      ],
      "metadata": {
        "id": "b3d81oOLp6Dm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "exchange_data.index = exchange_data.index.to_timestamp()"
      ],
      "metadata": {
        "id": "MwY11R__pwu_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "plt.figure(figsize=(18, 4))\n",
        "plt.plot(dk1.index, dk1['PM10'], label='PM10')\n",
        "y.plot(title=\"exchange britsh to american money per weekly of 1980 to 1993\")\n",
        "plt.xlabel(\"Year\")\n",
        "plt.ylabel(\"Number of exchange britsh to american money\")\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "jUxoWQhxq7C3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##nikkie"
      ],
      "metadata": {
        "id": "8c34KyEapEyU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#nikkie\n",
        "nikkie225_data.head()\n",
        "nikkie225_data.tail()\n"
      ],
      "metadata": {
        "id": "EicmkTIzoref"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nikkie225_data.info()"
      ],
      "metadata": {
        "id": "JUwaSb3Ls1VN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(nikkie225_data.index)\n",
        "print(type(nikkie225_data.index))"
      ],
      "metadata": {
        "id": "mikY1RFJqEZF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#transformação da data normal para um dataindex\n",
        "dk1 = dk1_data.set_index('Date')"
      ],
      "metadata": {
        "id": "d69G2mo9tLbl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ordena o DataFrame pelo índice\n",
        "dk1 = dk1.sort_index()"
      ],
      "metadata": {
        "id": "AJ0rx6KWtcwj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nikkie225_data.index = nikkie225_data.index.to_timestamp()"
      ],
      "metadata": {
        "id": "LSbNXN_gqF2n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(18, 4))\n",
        "\n",
        "plt.plot(dk1.index, dk1['PM10'], label='PM10')\n",
        "y.plot(title=\"exchange britsh to american money per weekly of 1980 to 1993\")\n",
        "plt.xlabel(\"Year\")\n",
        "plt.ylabel(\"Number of exchange britsh to american money\")\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "E1tHbmzlrMgB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#porcentagem de dados nulos nos conjuntos"
      ],
      "metadata": {
        "id": "4BGGFRjYqEEU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#ver todos os conjuntos\n",
        "#Realiza o cálculo da porcentagem de dados nulos por coluna\n",
        "\n",
        "def isnull(df):\n",
        "  return df.isnull().sum()/df.shape[0]*100\n",
        "\n",
        "isnull(lynx_data)\n",
        "isnull(sunpot_data)\n",
        "isnull(exchange_data)\n",
        "isnull(nikkie225_data)\n"
      ],
      "metadata": {
        "id": "YMOoB8bdnX4E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Verificando e removendo duplicatas\n",
        "def isduplicate(df):\n",
        "  return df.drop_duplicates(subset='Date', keep='first', inplace=True)\n",
        "\n",
        "isduplicate(lynx_data)\n",
        "isduplicate(sunpot_data)\n",
        "isduplicate(exchange_data)\n",
        "isduplicate(nikkie225_data)\n"
      ],
      "metadata": {
        "id": "n3ze-ohJr1Cl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#preencher os dados ausentes com interpolação"
      ],
      "metadata": {
        "id": "2AdTVTllnC33"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "dk1['Linear_PM10'] = dk1['PM10'].interpolate(method='linear')\n",
        "dk1['Spline_order_3_PM10'] =dk1['PM10'].interpolate(method='spline', order=3)\n",
        "dk1['Time_PM10'] = dk1['PM10'].interpolate(method='time')\n",
        "\n",
        "# Métodos de interpolação\n",
        "methods = ['Linear_PM10', 'Spline_order_3_PM10', 'Time_PM10']\n",
        "\n",
        "# Configurando o tamanho da figura\n",
        "fig, axes = plt.subplots(len(methods), 1, figsize=(12, 6), sharex=True)\n",
        "\n",
        "# Criando gráficos individuais com títulos para cada método\n",
        "for i, method in enumerate(methods):\n",
        "    axes[i].plot(dk1.index, dk1[method], label=method)\n",
        "    axes[i].set_title(f\"Interpolation Method: {method.replace('_', ' ')}\")  # Título dinâmico\n",
        "    axes[i].set_ylabel('PM10 Value')\n",
        "    axes[i].legend()\n",
        "\n",
        "# Configurando o eixo X para todos os gráficos\n",
        "axes[-1].set_xlabel('Date')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "5CPMgsKN_hO9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "import numpy as np\n",
        "\n",
        "# Real values (garantindo que não contenham NaNs para as métricas)\n",
        "real_values = dk1['PM10'].fillna(0)  # Remove valores ausentes\n",
        "\n",
        "# Interpolated values (alinhando índices)\n",
        "interpolated_values_linear = dk1.loc[real_values.index, 'Linear_PM10']\n",
        "interpolated_values_spline = dk1.loc[real_values.index, 'Spline_order_3_PM10']\n",
        "interpolated_values_time = dk1.loc[real_values.index, 'Time_PM10']\n",
        "\n",
        "# Garantindo que as Séries estejam alinhadas e sem NaN\n",
        "assert real_values.shape == interpolated_values_linear.shape, \"As dimensões não coincidem!\"\n",
        "assert real_values.shape == interpolated_values_spline.shape, \"As dimensões não coincidem!\"\n",
        "assert real_values.shape == interpolated_values_time.shape,  \"As dimensões não coincidem!\"\n",
        "# MAE\n",
        "mae_linear = mean_absolute_error(real_values, interpolated_values_linear)\n",
        "mae_spline = mean_absolute_error(real_values, interpolated_values_spline)\n",
        "mae_time = mean_absolute_error(real_values, interpolated_values_time)\n",
        "# RMSE\n",
        "rmse_linear = np.sqrt(mean_squared_error(real_values, interpolated_values_linear))\n",
        "rmse_spline = np.sqrt(mean_squared_error(real_values, interpolated_values_spline))\n",
        "rmse_time = np.sqrt(mean_squared_error(real_values, interpolated_values_time))\n",
        "\n",
        "print(f\"Linear Interpolation - MAE: {mae_linear}, RMSE: {rmse_linear}\")\n",
        "print(f\"Spline Interpolation - MAE: {mae_spline}, RMSE: {rmse_spline}\")\n",
        "print(f\"Time Interpolation - MAE: {mae_time}, RMSE: {rmse_time}\")\n",
        "\n",
        "\n",
        "print(\"------------visualization------------\")\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "IOySFl9__hLj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#estacionalidade para todo os conjuntos"
      ],
      "metadata": {
        "id": "jBJK0dXsGvgW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Função para testar a estacionaridade de cada coluna\n",
        "def testar_estacionaridade(df):\n",
        "    resultados = {}\n",
        "    for coluna in df.columns:\n",
        "        resultado = adfuller(df[coluna])\n",
        "        resultados[coluna] = {\n",
        "            'ADF Statistic': resultado[0],\n",
        "            'p-value': resultado[1],\n",
        "            'Lags Used': resultado[2],\n",
        "            'Number of Observations Used': resultado[3],\n",
        "            'Critical Value (1%)': resultado[4]['1%'],\n",
        "            'Critical Value (5%)': resultado[4]['5%'],\n",
        "            'Critical Value (10%)': resultado[4]['10%']\n",
        "        }\n",
        "    return resultados\n",
        "\n",
        "# Testando a estacionaridade\n",
        "resultados = testar_estacionaridade(df)\n",
        "\n",
        "# Exibindo os resultados\n",
        "for coluna, res in resultados.items():\n",
        "    print(f\"Resultados para {coluna}:\")\n",
        "    for chave, valor in res.items():\n",
        "        print(f\"  {chave}: {valor}\")\n",
        "    print(\"\\n\")"
      ],
      "metadata": {
        "id": "WD4Rn_8g_hJO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform the Augmented Dickey-Fuller test on the original series\n",
        "result_original = adfuller(data[\"Close\"])\n",
        "\n",
        "print(f\"ADF Statistic (Original): {result_original[0]:.4f}\")\n",
        "print(f\"p-value (Original): {result_original[1]:.4f}\")\n",
        "\n",
        "if result_original[1] < 0.05:\n",
        "    print(\"Interpretation: The original series is Stationary.\\n\")\n",
        "else:\n",
        "    print(\"Interpretation: The original series is Non-Stationary.\\n\")\n",
        "\n",
        "# Apply first-order differencing\n",
        "data['Close_Diff'] = data['Close'].diff()\n",
        "\n",
        "# Perform the Augmented Dickey-Fuller test on the differenced series\n",
        "result_diff = adfuller(data[\"Close_Diff\"].dropna())\n",
        "print(f\"ADF Statistic (Differenced): {result_diff[0]:.4f}\")\n",
        "print(f\"p-value (Differenced): {result_diff[1]:.4f}\")\n",
        "if result_diff[1] < 0.05:\n",
        "    print(\"Interpretation: The differenced series is Stationary.\")\n",
        "else:\n",
        "    print(\"Interpretation: The differenced series is Non-Stationary.\")\n"
      ],
      "metadata": {
        "id": "vlYrTDLdQpiF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#função de estar estacionalizade de cada conjunto\n",
        "#Augmented Dickey-Fuller Test:\n"
      ],
      "metadata": {
        "id": "UR7GdBsIHb-H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#acf  e pacf"
      ],
      "metadata": {
        "id": "B0gjWIeKGwwc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def acf_pacf(series, lags, figsize=(15, 6), title_prefix=\"\"):\n",
        "    \"\"\"\n",
        "    Gera gráficos de ACF (Autocorrelação) e PACF (Autocorrelação Parcial).\n",
        "\n",
        "    Parâmetros:\n",
        "    - series: Série temporal (array ou pandas Series).\n",
        "    - lags: Número de lags (defasagens) para calcular as correlações.\n",
        "    - figsize: Tamanho da figura (largura, altura).\n",
        "    - title_prefix: Prefixo para os títulos dos gráficos (como o nome da coluna).\n",
        "    \"\"\"\n",
        "    fig, axes = plt.subplots(1, 2, figsize=figsize)\n",
        "\n",
        "    # ACF\n",
        "    sm.graphics.tsa.plot_acf(series, lags=lags, ax=axes[0])\n",
        "    axes[0].set_title(f\"{title_prefix} - ACF\")\n",
        "\n",
        "    # PACF\n",
        "    sm.graphics.tsa.plot_pacf(series, lags=lags, ax=axes[1])\n",
        "    axes[1].set_title(f\"{title_prefix} - PACF\")\n",
        "\n",
        "    # Ajustar layout\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "dPhtqP6dHWAS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "for column in df.columns:\n",
        "    acf_pacf(df[column], lags=30, title_prefix=f\"conjunto: {column}\")"
      ],
      "metadata": {
        "id": "V-B11-PlHV9C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#treinamento"
      ],
      "metadata": {
        "id": "kDJ54XTzMdiQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##treinamento\n",
        "train_data = df.loc['2017-01-01':'2017-10-31']  # Treinamento: Janeiro a Outubro de 2017\n",
        "\n",
        "test_data = df.loc['2017-11-01':'2017-12-31']   # Teste: Novembro e Dezembro de 2017"
      ],
      "metadata": {
        "id": "HTuBjVUhMbNH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#normalização"
      ],
      "metadata": {
        "id": "1zEbSmW_RFcO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scaler_pm10 = MinMaxScaler()\n",
        "\n",
        "scaler_pm10.fit(train_data['Time_PM10'].values.reshape(-1, 1))"
      ],
      "metadata": {
        "id": "PlTo_RSXMgW_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data_normalized = train_data.copy()\n",
        "\n",
        "train_data_normalized['Time_PM10'] = scaler_pm10.transform(train_data['Time_PM10'].values.reshape(-1, 1))\n",
        "\n",
        "test_data_normalized = test_data.copy()\n",
        "\n",
        "test_data_normalized['Time_PM10'] = scaler_pm10.transform(test_data['Time_PM10'].values.reshape(-1, 1))"
      ],
      "metadata": {
        "id": "bpsc5jbKMg2j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Para o conjunto de treinamento\n",
        "for column in train_data_normalized.columns:\n",
        "    acf_pacf(train_data_normalized[column], lags=30, title_prefix=f\"Treinamento: {column}\")"
      ],
      "metadata": {
        "id": "20-rIU-DRcdq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "train_pm10=train_data_normalized['Time_PM10']\n",
        "\n",
        "\n",
        "test_pm10 =test_data_normalized['Time_PM10']"
      ],
      "metadata": {
        "id": "4DduTeNWRc42"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from statsmodels.tsa.stattools import adfuller\n",
        "\n",
        "result = adfuller(train_so2_diff)\n",
        "print(f\"ADF Statistic: {result[0]}\")\n",
        "print(f\"p-value: {result[1]}\")\n",
        "\n",
        "if result[1] < 0.05:\n",
        "    print(\"A série diferenciada é estacionária.\")\n",
        "else:\n",
        "    print(\"A série ainda não é estacionária.\")"
      ],
      "metadata": {
        "id": "gq5usCQ-Rq5Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from statsmodels.tsa.seasonal import seasonal_decompose\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Iterar por cada coluna do DataFrame\n",
        "for coluna in df.columns:\n",
        "    print(f\"Analisando sazonalidade para a coluna: {coluna}\")\n",
        "    serie_temporal = df[coluna]\n",
        "\n",
        "    # Realizar decomposição sazonal (ajuste o período, ex.: 12 para dados mensais)\n",
        "    decomposicao = seasonal_decompose(serie_temporal, model='additive', period=12)\n",
        "\n",
        "    # Plotar os componentes\n",
        "    decomposicao.plot()\n",
        "    plt.suptitle(f\"Decomposição Sazonal - {coluna}\", y=1.02)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "VNmKTqEJR3Wg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##treinamento\n",
        "#fit APENAS NO TREINAMENTO"
      ],
      "metadata": {
        "id": "Dju9VSaRWAlj"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OQ_JUpKIV7QN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#normalização tranform  to train and also to test dataframe"
      ],
      "metadata": {
        "id": "aVsonZZoWGgd"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dKyVYdApV7Dv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#o que deve ser feito\n",
        "\"\"\"\n",
        "explicar como realizar o projeto descrito no artigo \"Sequence in Hybridization of Statistical and Intelligent Models in Time Series Forecasting\". O objetivo principal do projeto é comparar a performance de modelos híbridos de séries temporais, especificamente as abordagens sequenciais **Linear–Não Linear** e **Não Linear–Linear**, utilizando modelos estatísticos (ARIMA) e inteligentes (MLP/SVM).\n",
        "\n",
        "O projeto envolve a implementação e avaliação de duas sequências principais de modelagem:\n",
        "\n",
        "**1. Sequência Linear–Não Linear (Exemplo: ARIMA-MLP)**\n",
        "\n",
        "Esta abordagem começa modelando a parte linear da série temporal e depois modela a parte não linear restante nos resíduos.\n",
        "\n",
        "*   **Passo 1: Modelagem Linear (ARIMA) nos Dados Originais**\n",
        "    *   **Entrada:** A série temporal original (\"row data\").\n",
        "    *   **Objetivo:** Capturar e modelar os padrões lineares na série.\n",
        "    *   **Processo:** Aplique o ciclo padrão de modelagem ARIMA:\n",
        "        *   **Identificação:** Determine as ordens (p, d, q) do modelo ARIMA. Isso geralmente envolve primeiro tornar a série estacionária, tipicamente por **diferenciação** (`d` vezes). Ferramentas como o software ITSM podem ajudar na análise visual da série e na aplicação de transformações ou diferenciações. Use as funções de autocorrelação (ACF) e autocorrelação parcial (PACF) da série (diferenciada, se necessário) para sugerir os valores apropriados de p e q. O ITSM pode plotar ACF/PACF.\n",
        "        *   **Estimação:** Estime os parâmetros (φ, θ, σ²) do modelo ARIMA(p, d, q) escolhido. A estimação por Máxima Verossimilhança (Maximum Likelihood Estimation - MLE) é um método comum. ITSM oferece opções para estimação preliminar (como Yule-Walker, Burg) e MLE, incluindo uma função Autofit que busca o modelo com menor critério AICC.\n",
        "        *   **Verificação Diagnóstica:** Avalie a adequação do modelo ARIMA ajustado, principalmente analisando seus resíduos. Os resíduos devem se assemelhar a um ruído branco (série sem autocorrelação significativa). Embora o objetivo seja usar esses resíduos no próximo passo, verificar se o modelo linear é adequado ajuda a garantir que ele capturou a estrutura linear esperada.\n",
        "    *   **Saída:** Um modelo ARIMA ajustado e a série de resíduos gerada por este modelo (`êARIMA,t`). Estes resíduos são considerados como contendo os padrões não lineares.\n",
        "\n",
        "*   **Passo 2: Modelagem Não Linear (Exemplo: MLP) nos Resíduos do ARIMA**\n",
        "    *   **Entrada:** Os resíduos (`êARIMA,t`) do modelo ARIMA ajustado no Passo 1.\n",
        "    *   **Objetivo:** Modelar os padrões não lineares ou as relações remanescentes que o modelo ARIMA não conseguiu explicar.\n",
        "    *   **Processo:** Ajuste um modelo não linear, como uma Rede Neural Artificial do tipo Multilayer Perceptron (MLP) ou uma Support Vector Machine (SVM), aos resíduos do ARIMA. No caso da MLP, os resíduos ARIMA servem como dados de entrada para o treinamento. A estrutura da MLP (número de camadas, neurônios, lags de entrada) deve ser determinada, frequentemente por um processo de otimização, testando diferentes topologias e escolhendo aquela que minimiza o erro (e.g., MSE) em dados de teste.\n",
        "    *   **Saída:** Um modelo não linear treinado capaz de prever ou modelar os resíduos do ARIMA (`ĉ2`).\n",
        "\n",
        "*   **Passo 3: Previsão Combinada**\n",
        "    *   A previsão final (`f̂combined,t`) é obtida **somando a previsão do modelo ARIMA** (feita para a série original usando o modelo ajustado no Passo 1, referida como `ĉ1` na notação do artigo) **e a previsão do modelo não linear** (feita para os resíduos do ARIMA usando o modelo ajustado no Passo 2, referida como `ĉ2` na notação do artigo). Ou seja, `f̂combined,t = ĉ1 + ĉ2`.\n",
        "\n",
        "**2. Sequência Não Linear–Linear (Exemplo: MLP-ARIMA)**\n",
        "\n",
        "Esta abordagem inverte a ordem, primeiro modelando a parte não linear e depois a parte linear nos resíduos.\n",
        "\n",
        "*   **Passo 1: Modelagem Não Linear (Exemplo: MLP) nos Dados Originais**\n",
        "    *   **Entrada:** A série temporal original (\"row data\").\n",
        "    *   **Objetivo:** Capturar e modelar os padrões não lineares na série primeiro.\n",
        "    *   **Processo:** Ajuste um modelo não linear (MLP ou SVM) diretamente nos dados originais. Determine a estrutura ideal do modelo não linear.\n",
        "    *   **Saída:** Um modelo não linear treinado e a série de resíduos gerada por este modelo (`êMLP,t`).\n",
        "\n",
        "*   **Passo 2: Modelagem Linear (ARIMA) nos Resíduos do Modelo Não Linear**\n",
        "    *   **Entrada:** Os resíduos (`êMLP,t`) do modelo não linear ajustado no Passo 1. Estes resíduos são considerados como contendo os padrões lineares não capturados pelo modelo não linear.\n",
        "    *   **Objetivo:** Modelar os padrões lineares ou as relações remanescentes nos resíduos.\n",
        "    *   **Processo:** Aplique o ciclo padrão de modelagem ARIMA (Identificação, Estimação, Verificação Diagnóstica) aos resíduos do modelo não linear (`êMLP,t`). Isso inclui a diferenciação dos resíduos, se necessário, para atingir a estacionariedade antes de identificar e estimar o modelo ARMA para os resíduos.\n",
        "    *   **Saída:** Um modelo ARIMA ajustado para os resíduos do modelo não linear.\n",
        "\n",
        "*   **Passo 3: Previsão Combinada**\n",
        "    *   A previsão final é obtida **somando a previsão do modelo não linear** (feita para a série original usando o modelo ajustado no Passo 1, referida como `ĉ2` nesta sequência) **e a previsão do modelo ARIMA** (feita para os resíduos do modelo não linear usando o modelo ajustado no Passo 2, referida como `ĉ1` nesta sequência). A fórmula combinada é `f̂combined,t = ĉ1 + ĉ2`.\n",
        "\n",
        "**Avaliação e Comparação**\n",
        "\n",
        "Após implementar ambas as sequências e obter as previsões combinadas para um conjunto de dados de teste, compare a performance usando métricas de erro como MSE, MAE e MAPE, bem como a correlação entre valores reais e previstos. O artigo sugere que a sequência Não Linear–Linear (MLP-ARIMA ou SVM-ARIMA) tende a produzir resultados mais precisos.\n",
        "\n",
        "Em suma, o projeto envolve a aplicação sequencial de um modelo linear e um modelo não linear, em duas ordens diferentes, usando os resíduos da primeira etapa como entrada para a segunda. A implementação de cada modelo (ARIMA, MLP, SVM) segue os procedimentos padrão (como identificação, estimação e verificação para ARIMA), adaptados para a entrada específica (dados originais ou resíduos) em cada passo da sequência híbrida.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "qmsq33wPk2Fz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#https://github.com/liannewriting/YouTube-videos-public/tree/main/arima-model-time-series-prediction-python\n",
        "#https://www.youtube.com/watch?v=Rl_tMSc_wKo\n",
        "#https://medium.com/data-science/time-series-forecasting-with-arima-sarima-and-sarimax-ee61099e78f6\n",
        "#https://towardsdatascience.com/understanding-arima-time-series-modeling-d99cd11be3f8/\n",
        "#https://towardsdatascience.com/time-series-forecasting-using-auto-arima-in-python-bb83e49210cd\n",
        "#https://medium.com/@mouse3mic3/estimating-arima-and-sarima-coefficients-using-genetic-algorithm-03f24ab66589"
      ],
      "metadata": {
        "id": "FBcT-lmsm9tW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}