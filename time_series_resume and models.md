# üìä Projeto de An√°lise e Previs√£o com S√©ries Temporais

Com o avan√ßo da tecnologia, houve um significativo aumento na produ√ß√£o de dados em todo o mundo. Paralelamente, observou-se um crescimento nas aplica√ß√µes que utilizam esses dados, abrangendo diversas √°reas, como sa√∫de, ind√∫stria e monitoramento de temperatura/clima. Contudo, √© comum ocorrer a perda de dados ou a presen√ßa de dados faltantes, sendo esses ausentes/faltantes originados por diversos motivos, como erros de transmiss√£o, problemas de comunica√ß√£o, defeitos nos sensores e erros humanos, constituindo um desafio para aqueles que buscam realizar previs√µes e an√°lises.

O problema torna-se mais complexo e profundo quando falamos de dados faltantes em s√©ries temporais multivariadas, devido a fatores como sazonalidade, tend√™ncia, ru√≠do branco, entre outros, que s√£o elementos importantes a serem considerados na escolha de um m√©todo de imputa√ß√£o apropriado (Luo et al., 2018).

Em rela√ß√£o √†s s√©ries temporais, √© importante destacar que estas podem apresentar tend√™ncias, manifestando-se com crescimento ou decr√©scimo ao longo do tempo, de forma linear ou n√£o. Al√©m disso, podem exibir sazonalidades, caracterizadas por fatores sazonais fixos e com frequ√™ncia conhecida. Tamb√©m √© poss√≠vel que as s√©ries temporais apresentem dados c√≠clicos, os quais n√£o seguem uma frequ√™ncia definida. Uma maneira de visualizar a tend√™ncia e a sazonalidade em s√©ries temporais √© por meio de fun√ß√µes de autocorrela√ß√£o (conforme mencionado por Luo et al., 2018) e outras t√©cnicas de an√°lise espec√≠ficas.

As s√©ries temporais discretas s√£o observa√ß√µes feitas em intervalos de tempo fixos, em que o intervalo de observa√ß√µes, conhecido como \(t\), pertence a um conjunto discreto. Enquanto as s√©ries temporais cont√≠nuas s√£o observa√ß√µes obtidas continuamente atrav√©s de algum intervalo no tempo, em que \(T=[0,1]\).

De maneira geral, podemos dizer que o que √© feito √© observar e analisar um conjunto de dados durante um tempo.

As s√©ries temporais podem ser sazonais, ter tend√™ncia e podem ter ambos ao mesmo tempo. A tend√™ncia de uma s√©rie temporal indica seu comportamento a longo prazo, se ela est√° est√°vel, crescendo ou decrescendo. Podemos descrever um modelo com tend√™ncia em \(X_t = M_t + Y_t\), em que \(M_t\) √© o componente de tend√™ncia e pode ser estimado atrav√©s do m√©todo dos m√≠nimos quadrados ou OLS, podendo ser visto como \(M_t = a_0 + a_1 \cdot t + a_2 \cdot t^2\). Os valores de \(a\) s√£o ajustados para minimizar a fun√ß√£o de soma de todos os valores \(N\) no tempo 1 \((X_t - M_t)^2\). A tend√™ncia linear √© uma linha reta que sobe ou desce, a tend√™ncia constante √© uma linha reta na horizontal, enquanto a tend√™ncia quadr√°tica indica uma curva subindo ou descendo.

Dizemos que temos a sazonalidade que ocorre em intervalos de tempo mais f√°ceis de serem previstos, como \(X_t = S_t + Y_t\), em que \(S_t\) √© o componente de sazonalidade, resultando em uma fun√ß√£o peri√≥dica que pode ser obtida atrav√©s da soma das ondas senoidais.

J√° aqueles que possuem movimento c√≠clico tendem a ter intervalos de tempo irregulares, sendo o ciclo o aumento ou redu√ß√£o de frequ√™ncia sem intervalos fixos. Alguns exemplos s√£o as crises econ√¥micas.

Erros s√£o os restos/sobras que existem junto √†s tend√™ncias e sazonalidades, sendo dados que ocorrem por causas aleat√≥rias e n√£o s√£o explicados na s√©rie temporal, matematicamente.

Conceitos de s√©ries temporais:
- S√©ries estacion√°rias: a m√©dia e a vari√¢ncia se mant√™m constantes durante o tempo. Em princ√≠pio, s√©ries com tend√™ncia e sazonalidade n√£o s√£o estacion√°rias, enquanto uma s√©rie temporal sazonal pode ser estacion√°ria.

Existem testes estat√≠sticos para verificar a estacionariedade de uma s√©rie, entre eles:

- **Dickey-Fuller (ADF):** Testa a presen√ßa de raiz unit√°ria. Se rejeita a hip√≥tese nula (de n√£o-estacionariedade), a s√©rie √© estacion√°ria.
- **KPSS (Kwiatkowski-Phillips-Schmidt-Shin):** Ao contr√°rio do ADF, sua hip√≥tese nula √© que a s√©rie **√© estacion√°ria**.
- **Phillips-Perron (PP):** Variante mais robusta do teste Dickey-Fuller, ajustando para heterocedasticidade e autocorrela√ß√£o nos res√≠duos.


Tipos de modelos:
- Modelos univariados incluem os modelos que se baseiam em uma √∫nica s√©rie temporal hist√≥rica, como o ARIMA e modelos univariados de Box e Jenkins de 1970.

O modelo ARIMA √© um modelo autorregressivo integrado de m√©dias m√≥veis.


## üß† Conceitos Fundamentais

### Modelos Cl√°ssicos
- AR, MA, ARMA
- ARIMA, SARIMA, SARIMAX
- Representa√ß√£o espectral, filtros lineares
- Estima√ß√£o no tempo e na frequ√™ncia (Fourier, periodograma)

### Redes Neurais e Modelos Profundos
- LSTM, RNN, MLP
- Arquiteturas h√≠bridas (ex: SARIMA + LSTM)



## üß† Conceitos Fundamentais de S√©ries Temporais

Antes de mergulharmos nos modelos e na metodologia, √© crucial entender os conceitos fundamentais que permeiam a an√°lise de s√©ries temporais.

## O Que S√£o S√©ries Temporais?

Uma **s√©rie temporal** √© uma sequ√™ncia de dados coletados e indexados ao longo do tempo, organizados em ordem cronol√≥gica. Cada ponto na s√©rie corresponde a uma observa√ß√£o realizada em um instante ou per√≠odo espec√≠fico.

Esses dados podem representar uma vasta gama de fen√¥menos que evoluem com o tempo, incluindo:

* **Economia e Finan√ßas:** Pre√ßos de a√ß√µes, taxas de c√¢mbio, infla√ß√£o, Produto Interno Bruto (PIB), vendas de varejo.
* **Ci√™ncias Naturais:** Temperatura di√°ria, precipita√ß√£o pluviom√©trica, n√≠veis de rios, atividade solar.
* **Engenharia:** Tr√°fego de rede, consumo de energia, leituras de sensores em processos industriais.
* **Marketing e Vendas:** Demanda por produtos, visitas a websites, engajamento em redes sociais.
* **Sa√∫de:** N√∫mero de casos de doen√ßas ao longo do tempo, batimentos card√≠acos monitorados continuamente.

---

### Tipos de S√©ries Temporais

As s√©ries temporais podem ser classificadas principalmente em duas categorias, dependendo da natureza da sua indexa√ß√£o temporal:

* **S√©ries Temporais Discretas:** S√£o sequ√™ncias de observa√ß√µes realizadas em **intervalos de tempo fixos e distintos**. O conjunto de instantes de observa√ß√£o ($t$) pertence a um conjunto discreto, como dias, semanas, meses, anos, etc. Exemplos incluem o pre√ßo de uma a√ß√£o no fechamento de cada dia ou a m√©dia mensal de temperatura.

* **S√©ries Temporais Cont√≠nuas:** Representam observa√ß√µes que s√£o obtidas de forma **cont√≠nua** ao longo de um intervalo de tempo. Teoricamente, podemos ter uma observa√ß√£o para cada instante dentro de um dado per√≠odo, como a leitura cont√≠nua da temperatura de um forno ou o monitoramento constante de um eletrocardiograma. Matematicamente, o √≠ndice de tempo $T$ pertence a um intervalo cont√≠nuo, como $[0, 1]$ (representando um intervalo normalizado) ou um intervalo real $[a, b]$.

√â importante notar que, na pr√°tica, mesmo fen√¥menos cont√≠nuos s√£o frequentemente amostrados e analisados como s√©ries temporais discretas devido √†s limita√ß√µes de coleta e armazenamento de dados.

### Componentes de S√©ries Temporais

A an√°lise de s√©ries temporais frequentemente envolve a decomposi√ß√£o dos dados em seus componentes constituintes, que ajudam a entender os padr√µes subjacentes e a realizar previs√µes. Os principais componentes s√£o:

* **Tend√™ncia (Trend):** Representa a dire√ß√£o geral de longo prazo da s√©rie temporal. Indica se os dados est√£o consistentemente aumentando, diminuindo ou permanecendo est√°veis ao longo do tempo. A tend√™ncia pode ser linear, n√£o linear (por exemplo, quadr√°tica, exponencial) ou at√© mesmo mudar de dire√ß√£o ao longo da s√©rie.

    Podemos modelar uma tend√™ncia ($m_t$) de diversas formas. Uma **tend√™ncia linear** pode ser expressa como:
    $$m_t = a_0 + a_1 t$$
    onde $a_0$ √© o intercepto e $a_1$ √© a inclina√ß√£o, indicando a taxa de crescimento ou decrescimento.

    Uma **tend√™ncia polinomial de grau 2 (quadr√°tica)** √© dada por:
    $$m_t = a_0 + a_1 t + a_2 t^2$$
    que pode capturar curvaturas na tend√™ncia.

    Os coeficientes ($a_0, a_1, a_2, ...$) s√£o geralmente estimados utilizando o m√©todo dos **M√≠nimos Quadrados Ordin√°rios (OLS - Ordinary Least Squares)**, que busca minimizar a soma dos quadrados das diferen√ßas entre os valores observados ($x_t$) e a tend√™ncia modelada ($m_t$):
    $$\min_{a_0, a_1, ...} \sum_{t=1}^{N} (x_t - m_t)^2$$
    onde $N$ √© o n√∫mero total de observa√ß√µes.

* **Sazonalidade (Seasonality):** Refere-se a padr√µes que se repetem em intervalos de tempo fixos e conhecidos, geralmente dentro de um ano. Exemplos comuns incluem o aumento nas vendas de sorvetes no ver√£o ou o pico de compras no varejo durante as festas de fim de ano. A sazonalidade √© peri√≥dica e pode ser modelada utilizando fun√ß√µes peri√≥dicas, como a soma de ondas senoidais (como mencionado no material da UFPE).

    Um modelo simples com componente sazonal ($s_t$) pode ser:
    $$x_t = s_t + Y_t$$
    onde $Y_t$ representa os outros componentes (tend√™ncia, ciclo, erro). A componente sazonal $s_t$ √© tal que $s_t = s_{t+p}$, onde $p$ √© o per√≠odo da sazonalidade (por exemplo, $p=12$ para sazonalidade mensal em dados anuais).

* **Ciclos (Cyclical Patterns):** Representam flutua√ß√µes de longo prazo na s√©rie temporal, com dura√ß√£o geralmente maior que um ano. Ao contr√°rio da sazonalidade, os ciclos n√£o t√™m uma frequ√™ncia fixa e seus intervalos de tempo s√£o irregulares. Exemplos cl√°ssicos s√£o os ciclos econ√¥micos de expans√£o e recess√£o.

* **Ru√≠do (Erro ou Res√≠duo):** √â o componente aleat√≥rio da s√©rie temporal que n√£o pode ser explicado pelos outros componentes (tend√™ncia, sazonalidade, ciclo). Representa a variabilidade n√£o sistem√°tica ou eventos imprevis√≠veis que afetam os dados.

Um modelo aditivo comum para decompor uma s√©rie temporal √©:
$$x_t = Tend√™ncia_t + Sazonalidade_t + Ciclo_t + Ru√≠do_t$$
Ou, de forma mais simplificada:

$$x_t = T_t + S_t + C_t + \epsilon_t$$

### Conceitos Importantes em S√©ries Temporais

* **Estacionariedade:** Uma s√©rie temporal √© considerada **estacion√°ria** se suas propriedades estat√≠sticas fundamentais, como a m√©dia e a vari√¢ncia, permanecem constantes ao longo do tempo. Al√©m disso, a autocovari√¢ncia entre dois pontos da s√©rie depende apenas da dist√¢ncia (lag) entre eles, e n√£o do tempo absoluto em que as observa√ß√µes foram feitas.

    Em princ√≠pio, s√©ries temporais que apresentam tend√™ncia ou sazonalidade **n√£o s√£o estacion√°rias**, pois sua m√©dia (devido √† tend√™ncia) ou sua m√©dia em certos per√≠odos (devido √† sazonalidade) variam com o tempo. No entanto, uma s√©rie temporal com sazonalidade pode se tornar estacion√°ria ap√≥s a remo√ß√£o do componente sazonal (dessazonaliza√ß√£o).

* **Testes de Estacionariedade:** Existem testes estat√≠sticos formais para verificar se uma s√©rie temporal √© estacion√°ria. Alguns dos testes mais comuns incluem:
    * **Teste de Dickey-Fuller (e sua vers√£o aumentada, ADF):** Testa a presen√ßa de uma raiz unit√°ria na s√©rie, cuja presen√ßa sugere n√£o estacionariedade.
    * **Teste KPSS (Kwiatkowski-Phillips-Schmidt-Shin):** Testa a hip√≥tese nula de estacionariedade em torno de uma m√©dia ou de uma tend√™ncia determin√≠stica.
    * **Teste de Phillips-Perron:** Similar ao ADF, mas com uma abordagem diferente para lidar com a autocorrela√ß√£o nos erros.

A compreens√£o desses conceitos √© fundamental para a an√°lise e modelagem eficaz de s√©ries temporais, permitindo a realiza√ß√£o de previs√µes e a extra√ß√£o de insights significativos dos dados.

### Conceitos Estat√≠sticos Fundamentais
* **Processos Estoc√°sticos e S√©ries Temporais:** Um processo estoc√°stico √© uma sequ√™ncia de vari√°veis aleat√≥rias indexadas pelo tempo. Uma s√©rie temporal √© a realiza√ß√£o de um processo estoc√°stico.
* **Estacionariedade:** Propriedade onde as caracter√≠sticas estat√≠sticas (m√©dia, vari√¢ncia, autocovari√¢ncia) n√£o mudam ao longo do tempo. S√©ries com tend√™ncia ou sazonalidade geralmente n√£o s√£o estacion√°rias.
* **Fun√ß√£o de Autocovari√¢ncia:** Mede a correla√ß√£o entre valores da s√©rie em diferentes pontos no tempo (lags).
* **Espectro:** Representa√ß√£o da distribui√ß√£o da vari√¢ncia da s√©rie em diferentes frequ√™ncias.

---

# üìà An√°lise de S√©ries Temporais: Guia Completo de Autocorrela√ß√£o (ACF) e Correla√ß√£o Parcial (PACF)

A an√°lise de s√©ries temporais √© um campo vital para a compreens√£o e previs√£o de fen√¥menos que evoluem ao longo do tempo. Uma das caracter√≠sticas mais fundamentais e informativas de uma s√©rie temporal √© a sua **autocorrela√ß√£o**, que revela a estrutura de depend√™ncia interna entre seus valores em diferentes pontos no tempo. Este guia detalha o que s√£o Autocorrela√ß√£o (ACF) e Correla√ß√£o Parcial (PACF), como interpret√°-los e suas aplica√ß√µes pr√°ticas na modelagem.

---

## üìå 1. O Que √© Autocorrela√ß√£o?

A autocorrela√ß√£o mede a **correla√ß√£o linear** de uma vari√°vel com ela mesma em diferentes pontos no tempo. Ela nos fornece uma medida adimensional da associa√ß√£o linear entre os valores atuais de uma s√©rie e seus valores passados (defasados).

### ‚úÖ **1.1. Coeficiente de Autocorrela√ß√£o**

No eixo Y de um gr√°fico de autocorrela√ß√£o, temos o **coeficiente de correla√ß√£o**, que √© uma medida padronizada e varia entre -1 e 1:

* **Coeficiente = 0:** Indica que n√£o h√° correla√ß√£o linear entre os valores da s√©rie e seus valores defasados. Ou seja, as vari√°veis s√£o independentes linearmente para aquela defasagem.
* **Coeficiente > 0 (pr√≥ximo a +1):** Indica uma **correla√ß√£o positiva**. Se o valor atual da s√©rie aumenta, o valor defasado correspondente tamb√©m tende a aumentar.
* **Coeficiente < 0 (pr√≥ximo a -1):** Indica uma **correla√ß√£o negativa**. Se o valor atual da s√©rie aumenta, o valor defasado correspondente tende a diminuir.

√â importante ressaltar que a presen√ßa de correla√ß√£o pode variar significativamente entre diferentes defasagens. Um atraso pode n√£o apresentar correla√ß√£o, enquanto atrasos maiores podem revelar uma depend√™ncia estatisticamente significativa. Os gr√°ficos de autocorrela√ß√£o nos ajudam a visualizar esses padr√µes.

### üî¨ **1.2. F√≥rmulas Fundamentais**

A compreens√£o das f√≥rmulas subjacentes ao c√°lculo da autocorrela√ß√£o e autocovari√¢ncia √© essencial:

* **Coeficiente de Autocorrela√ß√£o $r_k$:**
    Mede a rela√ß√£o entre $y_t$ (valor atual) e $y_{t-k}$ (valor defasado em $k$ per√≠odos). Para cada defasagem $k$, $r_k$ √© o coeficiente de correla√ß√£o de Pearson.

    $$r_k = \frac{\sum_{t=k+1}^{T} (y_t - \bar{y})(y_{t-k} - \bar{y})}{\sum_{t=1}^{T} (y_t - \bar{y})^2}$$

    Onde:
    * $T$: √â o comprimento da s√©rie temporal.
    * $\bar{y}$: √â a m√©dia da s√©rie.
    * $k$: √â a defasagem (lag).

    Os coeficientes de autocorrela√ß√£o para diferentes valores de $k$ comp√µem a **Fun√ß√£o de Autocorrela√ß√£o (ACF)**.

* **Autocovari√¢ncia $\gamma_k$:**
    Mede o grau de depend√™ncia linear que uma observa√ß√£o $Z_t$ possui com ela mesma defasada em $k$ per√≠odos.

    $$\gamma_k = \text{Cov}(Z_t, Z_{t-k}) = E[(Z_t - \mu)(Z_{t-k} - \mu)]$$

    Onde:
    * $\mu$: √â a m√©dia da s√©rie.

    A autocorrela√ß√£o depende apenas do intervalo entre as medidas ($h$ ou $k$), n√£o do ponto espec√≠fico no tempo.

---

## üéØ 2. Defasagem (Lag)

Uma **defasagem (lag)** representa o n√∫mero de per√≠odos de tempo que separam duas observa√ß√µes em uma s√©rie temporal.

* **Exemplo:** Se voc√™ est√° analisando dados mensais:
    * **Lag 1:** Rela√ß√£o entre o m√™s atual e o m√™s anterior.
    * **Lag 2:** Rela√ß√£o entre o m√™s atual e dois meses atr√°s.
    * E assim por diante.

---

## üìà 3. Autocorrela√ß√£o (ACF) vs. Correla√ß√£o Parcial (PACF)

Embora ambas as fun√ß√µes analisem a depend√™ncia da s√©rie com seus pr√≥prios valores passados, elas o fazem de maneiras distintas, fornecendo informa√ß√µes complementares.

### ‚úÖ **3.1. Fun√ß√£o de Autocorrela√ß√£o (ACF)**

* **Mede:** A correla√ß√£o entre uma vari√°vel ($X_t$) e seus valores defasados ($X_{t-k}$), abrangendo **toda a correla√ß√£o** ‚Äî tanto a direta quanto a indireta (aquela que √© transmitida atrav√©s de lags intermedi√°rios).
* **Intui√ß√£o:** Pense na ACF como capturando a "teia completa" de influ√™ncias passadas, onde a correla√ß√£o no lag 5, por exemplo, pode ser influenciada por correla√ß√µes nos lags 1, 2, 3 e 4.

### ‚úÖ **3.2. Fun√ß√£o de Autocorrela√ß√£o Parcial (PACF)**

* **Mede:** A correla√ß√£o **direta** entre uma vari√°vel ($X_t$) e seus valores defasados ($X_{t-k}$), **ap√≥s remover o efeito** de todas as defasagens intermedi√°rias ($X_{t-1}, X_{t-2}, \dots, X_{t-(k-1)}$).
* **Intui√ß√£o:** A PACF "limpa" essa teia, isolando a influ√™ncia direta de cada lag. Por exemplo, a PACF de lag 2 mede a correla√ß√£o entre $X_t$ e $X_{t-2}$ *somente* aquela n√£o explicada pela influ√™ncia de $X_{t-1}$.

---

## üß™ 4. Como Interpretar os Gr√°ficos ACF e PACF

Os gr√°ficos de ACF e PACF s√£o visualiza√ß√µes cruciais para a an√°lise. Eles apresentam os coeficientes de correla√ß√£o (ou correla√ß√£o parcial) no **Eixo Y** e as defasagens (lags) no **Eixo X**.

### ‚úÖ **4.1. Faixa Azul (Intervalo de Confian√ßa)**

* A **faixa azul** que circunda o zero nos gr√°ficos representa o **intervalo de confian√ßa** (tipicamente de 95%).
* **Signific√¢ncia Estat√≠stica:**
    * Se uma barra (pico) de correla√ß√£o **ultrapassa** essa faixa, o coeficiente de correla√ß√£o para aquela defasagem √© considerado **estatisticamente significativo**, ou seja, √© improv√°vel que seja zero por acaso.
    * Se os picos **ficam dentro** da faixa azul, a correla√ß√£o para aquela defasagem **n√£o √© estatisticamente significativa**. Isso sugere que a correla√ß√£o observada pode ser apenas ru√≠do aleat√≥rio e n√£o uma depend√™ncia real da s√©rie.

### üß† **4.2. Comportamento e Precis√£o**

* A precis√£o da autocorrela√ß√£o tende a diminuir √† medida que o n√∫mero de defasagens (lags) aumenta, pois h√° menos pares de dados para calcular a correla√ß√£o em defasagens maiores.

---

## ‚ö™ 5. Ru√≠do Branco

Um conceito fundamental na an√°lise de s√©ries temporais √© o de **ru√≠do branco**.

* **Defini√ß√£o:** Uma s√©rie de **ru√≠do branco** √© caracterizada por valores que s√£o independentes e identicamente distribu√≠dos, o que significa que n√£o h√° autocorrela√ß√£o linear em nenhum lag.
* **Interpreta√ß√£o nos Gr√°ficos:**
    * Para uma s√©rie de ru√≠do branco, espera-se que **todos (ou quase todos)** os coeficientes de autocorrela√ß√£o (tanto na ACF quanto na PACF) estejam **dentro da faixa azul** para todos os lags.
    * Os valores dos coeficientes ser√£o pr√≥ximos de zero, mas n√£o exatamente zero devido √†s varia√ß√µes aleat√≥rias inerentes aos dados.
* **Regra Pr√°tica:** Espera-se que aproximadamente 95% dos picos da ACF e PACF caiam dentro da faixa de $\pm 2/\sqrt{T}$, onde $T$ √© o tamanho da s√©rie temporal.

---

Recursos de defasagem e recursos de janela
Os recursos de defasagem s√£o valores em intervalos de tempo anteriores que s√£o considerados √∫teis porque s√£o criados com base na suposi√ß√£o de que o que aconteceu no passado pode influenciar ou conter um tipo de informa√ß√£o intr√≠nseca sobre o futuro. Por exemplo, pode ser vantajoso gerar recursos para vendas que ocorreram em dias anteriores √†s 16h00 se voc√™ quiser prever vendas semelhantes √†s 16h00 do dia seguinte.

Uma categoria interessante de recursos de defasagem √© chamada de recursos de defasagem aninhados. Para criar recursos de defasagem aninhados, os cientistas de dados devem identificar um per√≠odo de tempo fixo no passado e, em seguida, agrupar os valores dos recursos por esse per√≠odo de tempo - por exemplo, o n√∫mero de itens vendidos nas duas horas anteriores, nos tr√™s dias anteriores e na semana anterior.
A opera√ß√£o de adicionar recursos de defasagem √© chamada de m√©todo de janela deslizante ou recursos de janela. O exemplo acima mostra como aplicar um m√©todo de janela deslizante com uma largura de janela de oito. Os recursos de janela s√£o um resumo dos valores em uma janela fixa de intervalos de tempo anteriores.

Dependendo do cen√°rio da s√©rie temporal, voc√™ pode expandir a largura da janela e incluir mais recursos defasados. Uma pergunta comum que os cientistas de dados fazem antes de realizar a opera√ß√£o de adicionar recursos de defasagem √© o tamanho da janela. Uma boa abordagem seria criar uma s√©rie de diferentes larguras de janela e, alternativamente, adicion√°-las e remov√™-las do conjunto de dados para ver qual delas tem um efeito positivo mais evidente no desempenho do modelo.




## üìö 6. Aplica√ß√µes Pr√°ticas na Sele√ß√£o de Modelos ARIMA

Os gr√°ficos ACF e PACF s√£o as ferramentas prim√°rias e mais intuitivas para determinar os par√¢metros $p$, $d$, e $q$ de um modelo ARIMA (AutoRegressive Integrated Moving Average), ou seus an√°logos sazonais para SARIMA.

1.  ### ‚úÖ **Estacionariedade e o Par√¢metro `d` (Ordem de Diferencia√ß√£o)**
    * Antes de analisar os gr√°ficos ACF e PACF para $p$ e $q$, √© crucial que a s√©rie temporal seja **estacion√°ria** (m√©dia, vari√¢ncia e autocovari√¢ncia constantes ao longo do tempo).
    * Se a s√©rie n√£o for estacion√°ria (apresentar tend√™ncia, sazonalidade, etc.), ela precisa ser **diferenciada**. O n√∫mero de vezes que a s√©rie √© diferenciada para se tornar estacion√°ria define o par√¢metro `d` do ARIMA.
    * Os gr√°ficos ACF e PACF s√£o ent√£o analisados na s√©rie *diferenciada* para identificar $p$ e $q$.

2.  ### ‚úÖ **An√°lise dos Gr√°ficos para `p` e `q`**

    * **PACF para $p$ (Ordem Autorregressiva - AR):**
        * O par√¢metro $p$ est√° associado aos termos autorregressivos (AR) do modelo, que utilizam observa√ß√µes passadas da pr√≥pria s√©rie.
        * Na PACF, procure um "corte" abrupto (as barras significativas tornam-se n√£o significativas ou caem dentro da faixa azul) ap√≥s um certo n√∫mero de lags.
        * O n√∫mero do √∫ltimo lag significativo antes do corte sugere o valor de $p$.
        * **Exemplo:** Se a PACF tem picos significativos em lags 1, 2, mas em 3 e al√©m eles s√£o insignificantes, isso sugere $p=2$.

    * **ACF para $q$ (Ordem da M√©dia M√≥vel - MA):**
        * O par√¢metro $q$ est√° associado aos termos de m√©dia m√≥vel (MA) do modelo, que utilizam erros passados (ru√≠do branco) da previs√£o.
        * Na ACF, procure um "corte" abrupto (as barras significativas tornam-se n√£o significativas ou caem dentro da faixa azul) ap√≥s um certo n√∫mero de lags.
        * O n√∫mero do √∫ltimo lag significativo antes do corte sugere o valor de $q$.
        * **Exemplo:** Se a ACF tem um pico significativo apenas em lag 1, mas em 2 e al√©m os picos s√£o insignificantes, isso sugere $q=1$.

3.  ### ‚úÖ **Estima√ß√£o e Avalia√ß√£o do Modelo**

    * Ap√≥s a identifica√ß√£o inicial de $p, d, q$, o modelo ARIMA √© estimado.
    * Utilizam-se m√©todos como a **Estimativa de M√°xima Verossimilhan√ßa (MLE)** ou crit√©rios de informa√ß√£o como **AIC (Akaike Information Criterion)** e **BIC (Bayesian Information Criterion)** para refinar a escolha dos par√¢metros e comparar diferentes modelos.

4.  ### ‚úÖ **Valida√ß√£o do Modelo**

    * √â crucial testar e validar o modelo final. Isso √© feito principalmente atrav√©s da an√°lise dos **res√≠duos do modelo**.
    * Testes de diagn√≥stico, como o **Teste de Ljung-Box**, s√£o usados para verificar se os res√≠duos do modelo s√£o ru√≠do branco. Se os res√≠duos n√£o forem ru√≠do branco, isso indica que o modelo n√£o capturou toda a estrutura de depend√™ncia da s√©rie, e uma nova otimiza√ß√£o dos par√¢metros pode ser necess√°ria.

---

### ‚úÖ **Conclus√£o**

Os gr√°ficos ACF e PACF s√£o as **ferramentas diagn√≥sticas mais fundamentais e poderosas** na caixa de ferramentas de um analista de s√©ries temporais. Eles n√£o apenas revelam a quantidade de depend√™ncia linear entre uma observa√ß√£o e seus valores passados, mas tamb√©m a natureza direta dessa depend√™ncia. Dominar a interpreta√ß√£o desses gr√°ficos √© essencial para identificar o tipo e a ordem do modelo ARIMA ideal e para validar sua adequa√ß√£o.

---
### Modelos Cl√°ssicos
* **Modelos AR (Autoregressivos):** Utilizam valores passados da pr√≥pria s√©rie para prever o valor futuro.
* **Modelos MA (M√©dias M√≥veis):** Utilizam erros de previs√£o passados para prever o valor futuro.
* **Modelos ARMA (Autoregressive Moving Average):** Combinam componentes AR e MA.
* **Modelos ARIMA (Autoregressive Integrated Moving Average):** Estendem ARMA para s√©ries n√£o estacion√°rias atrav√©s da diferencia√ß√£o (componente "Integrated").
    * **O que √© o modelo ARIMA:** Combina AutoRegress√£o (AR), Integra√ß√£o (I - para tornar a s√©rie estacion√°ria por diferencia√ß√£o), e M√©dia M√≥vel (MA).
* **Modelos SARIMA (Seasonal ARIMA):** Extens√£o do ARIMA para dados com sazonalidade, incorporando componentes sazonais. √â um modelo multiplicativo que considera varia√ß√µes sazonais e n√£o sazonais.
* **Modelos SARIMAX:** Extens√£o do SARIMA que inclui vari√°veis ex√≥genas (preditores externos).
* **Representa√ß√£o Espectral e Filtros Lineares:** An√°lise da s√©rie no dom√≠nio da frequ√™ncia e aplica√ß√£o de filtros para extrair ou remover certas componentes.
* **Estima√ß√£o no Tempo e na Frequ√™ncia:**
    * **Dom√≠nio do Tempo:** Estima√ß√£o de par√¢metros dos modelos ARMA/ARIMA usando m√©todos como m√°xima verossimilhan√ßa.
    * **Dom√≠nio da Frequ√™ncia:** Utiliza√ß√£o da Transformada de Fourier e do periodograma para analisar a estrutura espectral da s√©rie e estimar componentes peri√≥dicas.

### Modelos de Aprendizado de M√°quina
* **SVM (Support Vector Machines):** Algoritmo supervisionado para classifica√ß√£o e regress√£o. Em s√©ries temporais, pode ser usado para prever o valor futuro como um problema de regress√£o.
    * Utiliza um hiperplano ideal para maximizar a margem entre classes (em classifica√ß√£o) ou ajustar uma fun√ß√£o para prever valores (em regress√£o).
    * **Linear SVMs:** Adequadas para dados linearmente separ√°veis.
    * **Kernel Trick:** Permite lidar com dados n√£o linearmente separ√°veis, transformando-os em espa√ßos de maior dimens√£o.
    * **Decision Boundary:** A superf√≠cie que separa as diferentes classes (em classifica√ß√£o).
* **Redes Neurais e Modelos Profundos:**
    * **MLP (Multilayer Perceptron):** Redes neurais feedforward b√°sicas.
    * **RNN (Redes Neurais Recorrentes):** Projetadas para processar dados sequenciais, utilizando loops para manter informa√ß√µes de passos anteriores.
        * **O que s√£o RNNs:** Usam loops para persistir informa√ß√µes ao longo da sequ√™ncia, tornando-as adequadas para dados sequenciais e s√©ries temporais.
        * Arquitetura b√°sica com entrada $x^{(t)}$, estado escondido $h^{(t)}$, e pesos $U, W, V$ compartilhados ao longo do tempo.
        * $h^{(t)} = f(U x^{(t)} + W h^{(t-1)})$, onde $f$ √© uma fun√ß√£o de ativa√ß√£o n√£o linear (tanh, ReLU).
    * **LSTM (Long Short-Term Memory):** Um tipo de RNN que utiliza mecanismos de gating para lidar com depend√™ncias de longo prazo e superar o problema do vanishing/exploding gradient.
        * **O que √© LSTM:** Uma RNN avan√ßada que usa backpropagation atrav√©s do tempo e c√©lulas de mem√≥ria para aprender depend√™ncias de longo prazo em dados sequenciais, mitigando os problemas de gradientes.
        * **Problema do Vanishing/Exploding Gradient:** Ocorre durante o backpropagation em redes profundas, onde os gradientes podem diminuir ou aumentar exponencialmente, dificultando o aprendizado.
* **Arquiteturas H√≠bridas:** Combina√ß√£o de diferentes modelos (estat√≠sticos e de aprendizado de m√°quina) para aproveitar seus pontos fortes (ex: SARIMA + LSTM).

## ü§ñ Otimiza√ß√£o de Modelos

### Algoritmos Evolutivos
Utiliza√ß√£o de algoritmos de otimiza√ß√£o para encontrar os melhores hiperpar√¢metros ou arquiteturas de modelos:
* **PSO (Particle Swarm Optimization):** Algoritmo de otimiza√ß√£o inspirado no comportamento social de p√°ssaros ou peixes, utilizando um enxame de part√≠culas que exploram o espa√ßo de busca.

## üî¨ Metodologia Proposta

### Objetivo Geral
Avaliar e comparar diferentes modelos de previs√£o em s√©ries temporais com foco em desempenho, robustez e efici√™ncia computacional.

### Etapas
1.  **Contextualiza√ß√£o do Problema:** Defini√ß√£o clara do problema de previs√£o a ser abordado e sua relev√¢ncia.
2.  **Objetivos Geral e Espec√≠ficos:** Detalhamento do objetivo principal e dos objetivos menores que o sustentam.
3.  **Trabalhos Relacionados:** Revis√£o da literatura existente sobre o problema e as abordagens de solu√ß√£o.
4.  **Perguntas de Pesquisa:** Formula√ß√£o das quest√µes que o projeto busca responder.
5.  **Metodologia Proposta:** Descri√ß√£o detalhada dos modelos e t√©cnicas que ser√£o utilizados.
6.  **Metodologia Experimental:**
    * **Arquitetura Utilizada:** Especifica√ß√£o das arquiteturas dos modelos implementados (ex: n√∫mero de camadas LSTM, ordem ARIMA).
    * **Organiza√ß√£o do Treinamento/Teste:** Como os dados ser√£o divididos para treinamento, valida√ß√£o e teste.
    * **Hiperpar√¢metros e Otimiza√ß√µes:** Detalhes dos hiperpar√¢metros dos modelos e as t√©cnicas de otimiza√ß√£o utilizadas (ex: otimizadores, fun√ß√µes de perda, taxas de aprendizado).
    * **M√©tricas Utilizadas:** Defini√ß√£o das m√©tricas de avalia√ß√£o de desempenho (ex: MSE, RMSE, MAE, MAPE) com suas f√≥rmulas e poss√≠veis representa√ß√µes gr√°ficas.
    * **Compara√ß√£o de Desempenho:** Estrat√©gia para comparar o desempenho dos diferentes modelos em termos de tempo de treinamento, erro de previs√£o e visualiza√ß√£o dos resultados.

# üìä An√°lise e Modelagem de S√©ries Temporais

Claro! Aqui est√° o seu conte√∫do reorganizado e estruturado em um formato de Markdown mais limpo, com t√≠tulos hier√°rquicos consistentes, separa√ß√£o clara entre se√ß√µes e uso padronizado de √≠cones e t√≥picos:

---

# üìä An√°lise e Modelagem de S√©ries Temporais

Perfeito, voc√™ listou diversas etapas importantes de uma pipeline completa de **modelagem e previs√£o de s√©ries temporais**. Abaixo, reorganizei esse conte√∫do em formato de **checklist estruturado em Markdown**, com categorias claras para facilitar a leitura, entendimento e execu√ß√£o.

---

# üìä Pipeline Completo para Modelagem de S√©ries Temporais

## 1. üì• Carregamento e Limpeza dos Dados

* [ ] Carregar os dados
* [ ] **Remover colunas n√£o pertinentes**
* [ ] **Remover linhas duplicadas**
* [ ] Converter colunas de data para `datetime`
* [ ] Definir coluna de data como **√≠ndice** e garantir **ordem cronol√≥gica**

---

## 2. üîç An√°lise Explorat√≥ria (EDA)

* [ ] Verificar **correla√ß√£o entre vari√°veis** (caso multivariada) ‚Üí `heatmap`
* [ ] Verificar **comportamento de momentum**
* [ ] Analisar presen√ßa de **white noise** (ru√≠do branco)
* [ ] Realizar **decomposi√ß√£o da s√©rie**

  * [ ] Estrat√©gia de decomposi√ß√£o (Ex: **exponencial**)
  * [ ] Visualizar **tend√™ncia** e **sazonalidade**
* [ ] Verificar e **interpolar dados faltantes**
* [ ] Analisar **estacionalidade** com:

  * [ ] **ACF (Fun√ß√£o de Autocorrela√ß√£o)**
  * [ ] **PACF (Autocorrela√ß√£o Parcial)**
* [ ] Aplicar **diferencia√ß√£o** caso a s√©rie n√£o seja estacion√°ria

---

## 3. üßπ Pr√©-processamento e Transforma√ß√µes

* [ ] Realizar **normaliza√ß√£o** dos dados (MinMax, Z-score etc.)
* [ ] Criar **janelas deslizantes** para regress√£o:

  * [ ] Para o conjunto de **treinamento**
  * [ ] Para o conjunto de **teste**
* [ ] Separar **20% do treino para valida√ß√£o**
* [ ] Selecionar **lags mais relevantes** com base em an√°lise estat√≠stica ou feature importance

---

## 4. üìà Treinamento de Modelos

* [ ] Ajustar apenas com o **conjunto de treinamento**
* [ ] Executar fun√ß√£o de modelagem com os **par√¢metros selecionados**
* [ ] Treinar e ajustar o modelo (ex: **ARIMA**)

  * [ ] Fit do modelo ARIMA
  * [ ] Escolha dos melhores par√¢metros (AIC, BIC, grid search)
  * [ ] Previs√£o com o modelo ajustado
* [ ] Avaliar res√≠duos:

  * [ ] Plotar res√≠duos
  * [ ] Verificar se os res√≠duos s√£o ru√≠do branco
  * [ ] Avaliar previs√µes no conjunto de valida√ß√£o/teste

---

## 5. üß™ Avalia√ß√£o e Valida√ß√£o

* [ ] Avaliar previs√µes com m√©tricas:

  * RMSE, MAE, MAPE, etc.
* [ ] Validar o modelo no conjunto de valida√ß√£o
* [ ] Confirmar que os res√≠duos est√£o centrados e sem autocorrela√ß√£o

---

## 6. üîó Combina√ß√£o de Modelos (H√≠bridos)

* [ ] Testar combina√ß√£o de modelos:

  * **Modelo de pondera√ß√£o**
  * **M√©todo de fus√£o**
  * **KNN para combina√ß√£o**
  * **Combina√ß√£o ponderada (PTI?)**
* [ ] Estrat√©gias de sele√ß√£o din√¢mica:

  * **Dynamic Selection**
  * **Oracle Selection**
  * **Worst Selection**
* [ ] Avaliar se a combina√ß√£o melhora os resultados

---

## 7. üßæ Considera√ß√µes Finais

* O **res√≠duo** √© a diferen√ßa entre o valor real e o previsto pelo modelo

  * Avaliar no conjunto de **treinamento**
  * Avaliar no conjunto de **teste**

o residuo √© a diferen√ßa entre o valore real e o valor previsto pelo modelo X(arima)
isso no conjunto de treinamento
o residuo √© a diferen√ßa entre o valore real e o valor previsto pelo modelo X(arima)

---

## üìà An√°lise de Estacionariedade

### üî¨ Teste de Dickey-Fuller Aumentado (ADF)

O **ADF** verifica a presen√ßa de **raiz unit√°ria**, ou seja, se a s√©rie √© **n√£o-estacion√°ria**.

#### üß™ Hip√≥teses:

* **H‚ÇÄ (nula):** A s√©rie √© **n√£o-estacion√°ria**
* **H‚ÇÅ (alternativa):** A s√©rie √© **estacion√°ria**

#### üìä Interpreta√ß√£o:

* **p-valor < 0.05:** Rejeita-se H‚ÇÄ ‚Üí S√©rie **estacion√°ria**
* **p-valor ‚â• 0.05:** Falha em rejeitar H‚ÇÄ ‚Üí S√©rie **n√£o-estacion√°ria**

### üß™ Exemplo de C√≥digo com ADF

```python
from statsmodels.tsa.stattools import adfuller
import pandas as pd
import os

caminho = 'caminho_para_sua_pasta/Pernambuco'
resultados_adf = []

for arquivo in os.listdir(caminho):
    if arquivo.endswith('.csv'):
        df = pd.read_csv(os.path.join(caminho, arquivo))

        if df.shape[1] >= 2:
            nome_serie = arquivo.split('.')[0]
            serie = df.iloc[:, 1].dropna()

            try:
                adf_result = adfuller(serie)
                resultados_adf.append({
                    'arquivo': nome_serie,
                    'ADF Statistic': adf_result[0],
                    'p-value': adf_result[1],
                    'lags used': adf_result[2],
                    'nobs': adf_result[3],
                    'critical value (1%)': adf_result[4]['1%'],
                    'critical value (5%)': adf_result[4]['5%'],
                    'critical value (10%)': adf_result[4]['10%'],
                    'conclus√£o': 'Estacion√°ria' if adf_result[1] < 0.05 else 'N√£o estacion√°ria'
                })
            except Exception as e:
                print(f"Erro ao processar {nome_serie}: {e}")

df_resultados_adf = pd.DataFrame(resultados_adf)
print(df_resultados_adf)
```

---

# üìö Modelos de S√©ries Temporais

## üìâ Modelos Cl√°ssicos

### üîπ AR, MA, ARMA, ARIMA e Extens√µes

* **AR (Autoregressivo):** Usa valores passados para prever futuros.
* **MA (M√©dia M√≥vel):** Usa erros de previs√£o passados.
* **ARMA:** Combina AR e MA.
* **ARIMA:** Usa diferencia√ß√£o para lidar com n√£o-estacionariedade.
* **SARIMA:** Extens√£o do ARIMA para sazonalidade.
* **SARIMAX:** Inclui vari√°veis ex√≥genas (preditores externos).

### üîπ Dom√≠nios de Estima√ß√£o

* **Dom√≠nio do Tempo:** Estima√ß√£o por m√°xima verossimilhan√ßa (ex: modelos ARIMA).
* **Dom√≠nio da Frequ√™ncia:** An√°lise espectral com Transformada de Fourier, periodograma e filtros.

---

## ü§ñ Modelos de Aprendizado de M√°quina

### üîπ SVM (Support Vector Machines)

* Usado para classifica√ß√£o e **regress√£o** em s√©ries temporais.
* **Linear SVM:** Para dados linearmente separ√°veis.
* **Kernel Trick:** Para dados n√£o lineares.
* **Decision Boundary:** Superf√≠cie que separa classes ou define fun√ß√£o de regress√£o.

### üîπ Redes Neurais

* **MLP:** Feedforward com m√∫ltiplas camadas.

* **RNN:** Redes com mem√≥ria temporal:

  * Estado oculto: \$h^{(t)} = f(Ux^{(t)} + Wh^{(t-1)})\$
  * Fun√ß√µes de ativa√ß√£o: `tanh`, `ReLU`

* **LSTM (Long Short-Term Memory):**

  * Variante de RNN com mecanismos de mem√≥ria.
  * Lida com **depend√™ncias de longo prazo**
  * Resolve problemas de **vanishing/exploding gradients**

### üîπ Arquiteturas H√≠bridas

* Exemplo: **SARIMA + LSTM**
* Combinam pontos fortes de modelos estat√≠sticos e redes neurais

---

# üöÄ Otimiza√ß√£o de Modelos

## üî∏ Algoritmos Evolutivos

### PSO (Particle Swarm Optimization)

* Inspirado em enxames (p√°ssaros, peixes)
* Cada part√≠cula busca a melhor solu√ß√£o
* √ötil para otimizar hiperpar√¢metros (ex: redes, SVM, ARIMA)

---

# üß™ Metodologia Proposta

## üéØ Objetivo Geral

Avaliar e comparar diferentes modelos de previs√£o em s√©ries temporais quanto a:

* Desempenho
* Robustez
* Efici√™ncia computacional

## üß± Etapas do Projeto

1. **Contextualiza√ß√£o do Problema**
2. **Objetivos Geral e Espec√≠ficos**
3. **Trabalhos Relacionados**
4. **Perguntas de Pesquisa**
5. **Metodologia Proposta**
6. **Metodologia Experimental**

   * Arquiteturas dos modelos (ex: ordem do ARIMA, n√∫mero de camadas LSTM)
   * Estrat√©gia de treino/teste
   * Escolha de hiperpar√¢metros e otimizadores
   * M√©tricas (ex: MSE, RMSE, MAE, MAPE)
   * Compara√ß√£o de desempenho (gr√°ficos, tabelas, tempo de execu√ß√£o)

---


## üìà An√°lise de Estacionariedade com Teste de Dickey-Fuller Aumentado (ADF)

Ao trabalhar com s√©ries temporais, um passo essencial √© verificar se a s√©rie √© **estacion√°ria**. Modelos como **ARIMA** exigem que a s√©rie seja estacion√°ria ‚Äî isto √©, que suas propriedades estat√≠sticas como **m√©dia**, **vari√¢ncia** e **autocorrela√ß√£o** n√£o mudem ao longo do tempo.

### üîç Entendendo o Teste ADF

O **ADF (Augmented Dickey-Fuller)** √© um teste estat√≠stico que verifica a presen√ßa de uma **raiz unit√°ria**, ou seja, se a s√©rie √© **n√£o-estacion√°ria**.

#### üß™ Hip√≥teses do Teste:

- **H‚ÇÄ (nula):** A s√©rie √© **n√£o-estacion√°ria** (possui raiz unit√°ria).
- **H‚ÇÅ (alternativa):** A s√©rie √© **estacion√°ria** (n√£o possui raiz unit√°ria).

#### üìä Interpreta√ß√£o do p-valor:

- Se **p-valor < 0.05** (n√≠vel de signific√¢ncia comum), **rejeitamos H‚ÇÄ** ‚Üí A s√©rie √© **estacion√°ria**.
- Se **p-valor ‚â• 0.05**, **falhamos em rejeitar H‚ÇÄ** ‚Üí A s√©rie √© **n√£o-estacion√°ria**.

### üß™ Aplica√ß√£o do Teste ADF em V√°rios Arquivos

O trecho de c√≥digo abaixo percorre arquivos de s√©ries temporais em uma pasta, aplica o teste ADF e armazena os resultados:

```python
from statsmodels.tsa.stattools import adfuller
import pandas as pd
import os

# Caminho para os arquivos por estado
caminho = 'caminho_para_sua_pasta/Pernambuco'  # exemplo

# Lista para armazenar resultados
resultados_adf = []

# Iterar sobre os arquivos CSV da pasta
for arquivo in os.listdir(caminho):
    if arquivo.endswith('.csv'):
        df = pd.read_csv(os.path.join(caminho, arquivo))

        # Verifica se h√° uma coluna temporal e de dados
        if df.shape[1] >= 2:
            nome_serie = arquivo.split('.')[0]
            serie = df.iloc[:, 1].dropna()

            try:
                adf_result = adfuller(serie)

                resultados_adf.append({
                    'arquivo': nome_serie,
                    'ADF Statistic': adf_result[0],
                    'p-value': adf_result[1],
                    'lags used': adf_result[2],
                    'nobs': adf_result[3],
                    'critical value (1%)': adf_result[4]['1%'],
                    'critical value (5%)': adf_result[4]['5%'],
                    'critical value (10%)': adf_result[4]['10%'],
                    'conclus√£o': 'Estacion√°ria' if adf_result[1] < 0.05 else 'N√£o estacion√°ria'
                })
            except Exception as e:
                print(f"Erro ao processar {nome_serie}: {e}")

# Converter resultados para DataFrame
df_resultados_adf = pd.DataFrame(resultados_adf)
print(df_resultados_adf)



# An√°lise de Estacionariedade com Teste de Dickey-Fuller Aumentado (ADF)

Ao trabalhar com s√©ries temporais, um passo essencial √© verificar se a s√©rie √© **estacion√°ria**. Modelos como **ARIMA** exigem que a s√©rie seja estacion√°ria ‚Äî isto √©, que suas propriedades estat√≠sticas como **m√©dia**, **vari√¢ncia** e **autocorrela√ß√£o** n√£o mudem ao longo do tempo.

## üìå Teste de Dickey-Fuller Aumentado (ADF)

O **ADF (Augmented Dickey-Fuller)** √© um teste estat√≠stico que verifica a presen√ßa de uma **raiz unit√°ria**, ou seja, se a s√©rie √© **n√£o-estacion√°ria**.

### üß™ Hip√≥teses do Teste:

- **H‚ÇÄ (nula):** A s√©rie √© **n√£o-estacion√°ria** (possui raiz unit√°ria).
- **H‚ÇÅ (alternativa):** A s√©rie √© **estacion√°ria** (n√£o possui raiz unit√°ria).

### üìà Interpreta√ß√£o do p-valor:

- Se **p-valor < 0.05** (n√≠vel de signific√¢ncia comum), **rejeitamos H‚ÇÄ** ‚Üí A s√©rie √© **estacion√°ria**.
- Se **p-valor ‚â• 0.05**, **falhamos em rejeitar H‚ÇÄ** ‚Üí A s√©rie √© **n√£o-estacion√°ria**.

---

## üìÇ Aplicando o Teste ADF em Arquivos de S√©ries Temporais

O trecho de c√≥digo abaixo percorre os arquivos de s√©ries temporais, aplica o teste de Dickey-Fuller e armazena os resultados em um DataFrame.

```python
from statsmodels.tsa.stattools import adfuller
import pandas as pd
import os

# Caminho para os arquivos por estado
caminho = 'caminho_para_sua_pasta/Pernambuco'  # exemplo: pode ser alterado dinamicamente

# Lista para armazenar resultados
resultados_adf = []

# Iterar sobre os arquivos CSV da pasta
for arquivo in os.listdir(caminho):
    if arquivo.endswith('.csv'):
        df = pd.read_csv(os.path.join(caminho, arquivo))

        # Verifica se h√° uma coluna temporal e de dados
        if df.shape[1] >= 2:
            nome_serie = arquivo.split('.')[0]  # nome do arquivo sem extens√£o
            serie = df.iloc[:, 1].dropna()  # considera a segunda coluna como s√©rie

            try:
                adf_result = adfuller(serie)

                resultados_adf.append({
                    'arquivo': nome_serie,
                    'ADF Statistic': adf_result[0],
                    'p-value': adf_result[1],
                    'lags used': adf_result[2],
                    'nobs': adf_result[3],
                    'critical value (1%)': adf_result[4]['1%'],
                    'critical value (5%)': adf_result[4]['5%'],
                    'critical value (10%)': adf_result[4]['10%'],
                    'conclus√£o': 'Estacion√°ria' if adf_result[1] < 0.05 else 'N√£o estacion√°ria'
                })
            except Exception as e:
                print(f"Erro ao processar {nome_serie}: {e}")

# Converter resultados para DataFrame
df_resultados_adf = pd.DataFrame(resultados_adf)

# Exibir os resultados
print(df_resultados_adf)
---
O Teste ADF √© uma ferramenta fundamental na an√°lise de s√©ries temporais. Ele permite identificar se transforma√ß√µes como diferencia√ß√£o (differencing) s√£o necess√°rias antes de aplicar modelos como ARIMA ou SARIMA. Uma vez identificada a presen√ßa de n√£o estacionariedade, podemos aplicar transforma√ß√µes e reavaliar com o ADF at√© que a estacionariedade seja alcan√ßada.

----
An LSTM is more or less an RNN that uses additional weights to determine when its storage cells are written to, read from, or erased. In an RNN the cells are simply written to and read from every time.

So you need to store those additional weights if you are using an LSTM, and you also need additional operations to compute how ‚Äûopen‚Äú the cells are to being written and read. This ‚Äûopenness‚Äú is also stored as a so-called cell state alongside the contents of the cell, meaning you need more storage there as well.

With how fast computers are, it usually doesn‚Äôt make a huge difference, but if you stack a lot of LSTMs or have really long sequences, like, thousands of elements, it adds up.

However, the additional logic also makes LSTMs more powerful and stable. So there are often situations where you would need several RNN layers to replace a single LSTM layer - in which case the LSTM is more efficient.

About sequence length, I usually start with a length of 20, so your 19 is already as close as it gets. You should look into whether increasing it further also further improves performance though.

Edit: I‚Äòd say that typically, RNNs are stable enough to handle sequences of a few dozen to maybe around a hundred values. After that, you‚Äòll start to really see the benefits of LSTMs, which can process into the thousands of values. If you have a sequence length of, like, ten thousand values or beyond that, you should look into S5, H3 if what you‚Äôre doing needs associative recall of specific old sequence elements, or other sequence model architecture based on SSMs.
--- 
### üîó Refer√™ncias
* [Pr√©-processamento de S√©ries Temporais - 365 Data Science](https://365datascience.com/tutorials/time-series-analysis-tutorials/pre-process-time-series-data/)
* [Preprocessing para aprendizado supervisionado - Towards Data Science](https://towardsdatascience.com/preprocessing-time-series-data-for-supervised-learning-2e27493f44ae)
* [T√©cnica de Data Windowing - LinkedIn](https://www.linkedin.com/pulse/data-windowing-technique-used-time-series-forecasting-alejandro/)
* [Rolling Windows - Medium](https://medium.com/@karamel.itu/time-series-rolling-windows-5cf9ec500e83)
* [EDA em S√©ries Temporais - Medium](https://medium.com/data-and-beyond/mastering-exploratory-data-analysis-eda-everything-you-need-to-know-7e3b48d63a95)

## üîé Artigos Cient√≠ficos e Livros

### Livros Base
* *An√°lise de S√©ries Temporais* ‚Äì Morettin & Toloi
* *Econometria B√°sica* ‚Äì Damodar Gujarati
* *Elementos de Estat√≠stica Computacional* ‚Äì Frery & Cribari Neto
* *Machine Learning* ‚Äì Tom Mitchell ¬†
    [PDF](https://www.cs.cmu.edu/~tom/files/MachineLearningTomMitchell.pdf)

### Artigos do Prof. Fausto
* [IEEExplore 2014 - Previs√£o com H√≠brido PSO + SVM](https://ieeexplore.ieee.org/abstract/document/6974534)
* [Neurocomputing 2015 - H√≠brido com Wavelet](https://www.sciencedirect.com/science/article/abs/pii/S0925231215016057)
* [Knowledge-Based Systems 2019 - ELM Otimizado](https://www.sciencedirect.com/science/article/abs/pii/S0950705119301327)
* [IEEE 2021 - Forecast com PSO + Deep](https://ieeexplore.ieee.org/abstract/document/9340584)

### Outros Artigos
* [Forecasting with artificial neural networks:The state of the art](https://www.oscogen.ethz.ch/members/literature_restricted%20access/ann_for_001.pdf)


## ‚öôÔ∏è Modelos de Previs√£o

### üî∏ ARIMA e SARIMA
- [ARIMA do zero em Python - Medium](https://medium.com/analytics-vidhya/arima-model-from-scratch-in-python-489e961603ce)
- [Como o ARIMA funciona - Burak Ayy](https://burakayy.com/blog/how-arima-works)
- [Treinamento SARIMA passo a passo - MLPills](https://mlpills.dev/time-series/how-to-train-a-sarima-model-step-by-step/)
- [Compara√ß√£o ARIMA, SARIMA, SARIMAX - Towards Data Science](https://towardsdatascience.com/time-series-forecasting-with-arima-sarima-and-sarimax-ee61099e78f6/)
- [Forecasting com SARIMA - Medium](https://medium.com/@ozdogar/time-series-forecasting-using-sarima-python-8db28f1d8cfc)
https://www.eng.uwaterloo.ca/~kwhipel/Time%20Series%20Book.htm
https://www.datacamp.com/tutorial/arima

https://medium.com/@learn-simplified/build-arima-model-from-scratch-part-1-b72b73ba230f

https://www.datacamp.com/tutorial/arima

https://www.kaggle.com/code/phunghieu/arima-from-scratch



\subsection{Classical Statistical methods:}
For univariate time series, methods such as interpolation and autoregressive models, such as ARIMA~\cite{bartholomew1971time} and SARIMA~\cite{hamzaccebi2008improving}), are often more useful. However, when dealing with the complexity of MTS, they may not fully capture the relationships between different variables in the dataset.
####o que e o modelo arima

 √© um modelo estat√≠stico amplamente utilizado para previs√£o de s√©ries temporais.
 Ele combina tr√™s componentes principais:
AutoRegressivo (AR): A rela√ß√£o entre uma observa√ß√£o e um n√∫mero de lags de si mesma.
M√©dia M√≥vel (MA): A modelagem do erro como uma combina√ß√£o linear de erros anteriores.
Integra√ß√£o (I): Diferen√ßas sucessivas da s√©rie para torn√°-la estacion√°ria.


O modelo ARIMA √© um modelo autorregressivo integrado de m√©dias m√≥veis.



####o que √© o sarima
Quando h√° sazonalidade nos dados, o modelo SARIMA (Seasonal ARIMA) √© uma extens√£o do ARIMA, que incorpora componentes sazonais, combinando varia√ß√µes sazonais e n√£o sazonais. Ele √© conhecido como um modelo multiplicativo



### üî∏ SVM 
- √© um algoritmo supervicionado de aprendizado de maquina que classifica os dados , usando 
 linha ideal ou a partir do  hiper plano , que √© um conceito que generaliza a ideia de um plano no espa√ßo tridimensional para espa√ßos matem√°ticos de dimens√µes arbitr√°ria,  (optimal line) qmaximiza a distancias entre as classes em um espa√ßo N dimencional.

Os SVMs s√£o comumente usados em problemas de classifica√ß√£o. Elas distinguem entre duas classes encontrando o hiperplano ideal que maximiza a margem entre os pontos de dados mais pr√≥ximos de classes opostas. O n√∫mero de recursos nos dados de entrada determina se o hiperplano √© uma linha em um espa√ßo bidimensional ou um plano em um espa√ßo n-dimensional. Como v√°rios hiperplanos podem ser encontrados para diferenciar as classes, a maximiza√ß√£o da margem entre os pontos permite que o algoritmo encontre o melhor limite de decis√£o entre as classes. Isso, por sua vez, permite que ele generalize bem para novos dados e fa√ßa previs√µes de classifica√ß√£o precisas. As linhas adjacentes ao hiperplano ideal s√£o conhecidas como vetores de suporte, pois esses vetores passam pelos pontos de dados que determinam a margem m√°xima.
o algoritmo svm √© amplamente utilizado em aprendizado de maquina devido a sua qualidadade de linar com a tarefa de classifica√ßao lineares e nao lineares.
contudo quando os dados n√£o estao linearmente separados , as fun√ßoes de kernel sao usadas para transformar os dados higher-dimensional space to enable linear separation.
 This application of kernel functions can be known as the ‚Äúkernel trick‚Äù, and the choice of kernel function, such as linear kernels, polynomial kernels, radial basis function (RBF) kernels, or sigmoid kernels, depends on data characteristics and the specific use case.

<img scr ="https://www.ibm.com/content/dam/connectedassets-adobe-cms/worldwide-content/creative-assets/s-migr/ul/g/8f/27/3-1_svm_optimal-hyperplane_max-margin_support-vectors-2-1.png">

linear svms sao usadas com  linearly separable data, o que significa que os dados nao precisam de nenhuma forma serem transformados para separar os dados de diferentes classes. 

### üî∏The decision boundary

### üî∏ LSTM e Deep Learning
- [LSTM com Tensorflow/Keras - Curiousily](https://curiousily.com/posts/time-series-forecasting-with-lstms-using-tensorflow-2-and-keras-in-python/)
- [LSTM para COVID-19 - Curiousily](https://curiousily.com/posts/time-series-forecasting-with-lstm-for-daily-coronavirus-cases/)
- [Previs√£o de Demanda com LSTM - Curiousily](https://curiousily.com/posts/demand-prediction-with-lstms-using-tensorflow-2-and-keras-in-python/)
- [LSTM no YouTube (tutorial)](https://www.youtube.com/watch?v=c0k-YLQGKjY&t=1379s)
- [Exemplo pr√°tico de LSTM no Kaggle](https://www.kaggle.com/code/paulorzp/laborat-rio-12b-usando-lstm-em-s-ries-temporais)
- [Vantagens e Desvantagens do LSTM - Medium](https://medium.com/@prudhviraju.srivatsavaya/lstm-implementation-advantages-and-diadvantages-914a96fa0acb)
- [Previs√£o de a√ß√µes com LSTM - Medium](https://medium.com/datarisk-io/introdu%C3%A7%C3%A3o-%C3%A0s-redes-lstm-prevendo-valor-de-a%C3%A7%C3%B5es-na-bolsa-df270ca0cee5)

Com certeza! Vamos aprofundar a se√ß√£o sobre LSTM e Deep Learning para previs√£o de s√©ries temporais, tornando o conte√∫do mais robusto e confi√°vel.

### üî∏ LSTM e Deep Learning para S√©ries Temporais

Redes Neurais Recorrentes (RNNs) e, em particular, as Redes de Mem√≥ria de Longo Prazo (LSTMs), representam uma classe poderosa de modelos de aprendizado profundo que se destacam no processamento e previs√£o de dados sequenciais, como s√©ries temporais. Sua arquitetura intr√≠nseca permite que capturem depend√™ncias temporais complexas, tornando-as adequadas para modelar padr√µes de longo prazo em dados que evoluem ao longo do tempo.

#### Redes Neurais Recorrentes (RNNs)

As RNNs s√£o projetadas especificamente para lidar com sequ√™ncias de dados. Diferentemente das redes neurais feedforward tradicionais, as RNNs incorporam mecanismos de *feedback* que permitem que informa√ß√µes de passos anteriores na sequ√™ncia influenciem o processamento do passo atual. Essa capacidade de "mem√≥ria" torna as RNNs teoricamente capazes de aprender depend√™ncias temporais.

A arquitetura b√°sica de uma RNN envolve uma unidade recorrente que recebe a entrada atual e o estado oculto do passo anterior, produzindo um novo estado oculto e uma sa√≠da. Esse estado oculto atua como uma mem√≥ria, carregando informa√ß√µes relevantes sobre a hist√≥ria da sequ√™ncia.

No entanto, as RNNs tradicionais enfrentam desafios significativos ao aprender depend√™ncias de longo prazo devido ao problema do **vanishing gradient** (gradiente desaparecendo) e, em menor grau, ao **exploding gradient** (gradiente explodindo) durante o processo de treinamento com backpropagation atrav√©s do tempo (BPTT).

#### Redes de Mem√≥ria de Longo Prazo (LSTMs)

As LSTMs s√£o uma arquitetura de RNN que foi explicitamente projetada para mitigar os problemas do vanishing e exploding gradients, permitindo que a rede aprenda depend√™ncias de longo prazo de forma mais eficaz. Elas introduzem uma unidade de mem√≥ria mais complexa, chamada **c√©lula de mem√≥ria**, que √© capaz de manter informa√ß√µes por longos per√≠odos de tempo.

A c√©lula de mem√≥ria da LSTM √© controlada por tr√™s mecanismos de *gating* (port√µes):

* **Port√£o de Entrada (Input Gate):** Controla quais novas informa√ß√µes devem ser armazenadas na c√©lula de mem√≥ria.
* **Port√£o de Esquecimento (Forget Gate):** Controla quais informa√ß√µes existentes na c√©lula de mem√≥ria devem ser descartadas.
* **Port√£o de Sa√≠da (Output Gate):** Controla quais informa√ß√µes da c√©lula de mem√≥ria devem ser usadas para gerar o estado oculto e a sa√≠da da LSTM.

Esses port√µes utilizam fun√ß√µes sigm√≥ides para produzir valores entre 0 e 1, atuando como interruptores que modulam o fluxo de informa√ß√µes. A intera√ß√£o desses port√µes permite que a LSTM aprenda seletivamente a reter, atualizar e liberar informa√ß√µes ao longo da sequ√™ncia temporal.

**Vantagens das LSTMs para S√©ries Temporais:**

* **Captura de Depend√™ncias de Longo Prazo:** A principal vantagem das LSTMs √© sua capacidade de modelar rela√ß√µes temporais que se estendem por longas sequ√™ncias, crucial para muitas tarefas de previs√£o de s√©ries temporais.
* **Lidando com Dados Sequenciais Complexos:** As LSTMs podem aprender padr√µes complexos e n√£o lineares em dados de s√©ries temporais, incluindo sazonalidade, tend√™ncias e outros padr√µes intrincados.
* **Robustez a Ru√≠do:** As c√©lulas de mem√≥ria e os mecanismos de gating tornam as LSTMs relativamente robustas a ru√≠dos e irregularidades nos dados.
* **Flexibilidade na Arquitetura:** As LSTMs podem ser combinadas em arquiteturas profundas (m√∫ltiplas camadas LSTM) e integradas com outras camadas de redes neurais (como camadas densas) para modelar hierarquias de padr√µes nos dados.

**Desafios e Considera√ß√µes ao Usar LSTMs:**

* **Complexidade Computacional:** O treinamento de modelos LSTM pode ser computacionalmente intensivo e demorado, especialmente para sequ√™ncias longas e arquiteturas profundas.
* **Necessidade de Dados Significativos:** As LSTMs geralmente requerem uma quantidade consider√°vel de dados de treinamento de alta qualidade para aprender representa√ß√µes eficazes.
* **Ajuste de Hiperpar√¢metros:** A arquitetura da rede LSTM (n√∫mero de camadas, n√∫mero de unidades por camada) e os hiperpar√¢metros de treinamento (taxa de aprendizado, batch size, n√∫mero de √©pocas) precisam ser cuidadosamente ajustados para obter um bom desempenho.
* **Interpretabilidade:** Como outras redes neurais profundas, os modelos LSTM podem ser dif√≠ceis de interpretar, tornando a compreens√£o dos motivos por tr√°s de suas previs√µes um desafio.
* **Pr√©-processamento de Dados:** O desempenho das LSTMs √© altamente dependente do pr√©-processamento adequado dos dados de s√©ries temporais, incluindo normaliza√ß√£o, tratamento de valores faltantes e cria√ß√£o de sequ√™ncias de entrada apropriadas (janelamento).

**Deep Learning para S√©ries Temporais:**

As LSTMs s√£o apenas um componente do arsenal de modelos de aprendizado profundo para s√©ries temporais. Outras arquiteturas e t√©cnicas incluem:

* **RNNs com Mecanismos de Aten√ß√£o:** Permitem que o modelo se concentre nas partes mais relevantes da sequ√™ncia de entrada ao fazer previs√µes.
* **GRUs (Gated Recurrent Units):** Uma variante mais simplificada das LSTMs com menos par√¢metros.
* **CNNs (Redes Neurais Convolucionais) 1D:** Podem ser usadas para extrair padr√µes locais ao longo da dimens√£o temporal da s√©rie.
* **Modelos Transformer:** Originalmente desenvolvidos para processamento de linguagem natural, os Transformers t√™m mostrado resultados promissores em tarefas de previs√£o de s√©ries temporais, especialmente para depend√™ncias de longo alcance.
* **Modelos H√≠bridos:** A combina√ß√£o de diferentes arquiteturas de deep learning (por exemplo, CNNs + LSTMs) ou a integra√ß√£o de modelos estat√≠sticos tradicionais com redes neurais profundas (como mencionado na se√ß√£o de modelos h√≠bridos do seu projeto) pode levar a um melhor desempenho em certos cen√°rios.

A escolha da arquitetura de deep learning mais adequada para uma tarefa de previs√£o de s√©ries temporais espec√≠fica depende das caracter√≠sticas dos dados (comprimento da sequ√™ncia, sazonalidade, ru√≠do), da complexidade dos padr√µes a serem modelados e dos recursos computacionais dispon√≠veis.

**Confiabilidade das Fontes:**

As fontes listadas fornecem tutoriais pr√°ticos e exemplos de implementa√ß√£o de LSTMs usando bibliotecas populares como TensorFlow e Keras (Curiousily, Kaggle). Medium √© uma plataforma de blogs onde autores compartilham suas experi√™ncias e conhecimentos, o que pode ser √∫til para entender conceitos e obter exemplos pr√°ticos, mas a confiabilidade pode variar dependendo do autor. O tutorial no YouTube tamb√©m pode ser √∫til para uma introdu√ß√£o visual.

Para uma compreens√£o mais profunda e confi√°vel dos fundamentos te√≥ricos das LSTMs e do aprendizado profundo para s√©ries temporais, √© recomendado consultar livros acad√™micos, artigos de pesquisa revisados por pares e documenta√ß√£o oficial de bibliotecas de deep learning (TensorFlow, PyTorch).

Espero que esta expans√£o forne√ßa uma compreens√£o mais completa e confi√°vel do uso de LSTMs e deep learning para previs√£o de s√©ries temporais!

#### Redes Neurais

**O que √© uma rede neural artificial?**

Uma rede neural artificial √© um modelo computacional inspirado na estrutura e no funcionamento do c√©rebro humano, particularmente na forma como os neur√¥nios biol√≥gicos se comunicam. Essencialmente, s√£o algoritmos de reconhecimento de padr√µes que identificam regularidades em dados de entrada e atribuem significado a esses padr√µes.

**Neur√¥nios Artificiais e Conex√µes**

O bloco fundamental de uma rede neural artificial √© o neur√¥nio artificial, frequentemente modelado a partir do conceito de um perceptron. Um neur√¥nio artificial recebe m√∫ltiplos sinais de entrada, cada um associado a um peso (representado por um vetor). Esses sinais ponderados s√£o combinados atrav√©s de um produto escalar, ao qual se adiciona um valor de bias (que n√£o depende das entradas).

Em analogia com o neur√¥nio biol√≥gico ‚Äì uma c√©lula que conduz sinais el√©tricos em uma √∫nica dire√ß√£o, composta por um corpo celular (onde reside o n√∫cleo), um ax√¥nio (que transmite o sinal) e dendritos (que conectam o neur√¥nio a outros) ‚Äì o neur√¥nio artificial opera de forma semelhante. Para que um neur√¥nio biol√≥gico dispare um sinal, ele precisa receber um est√≠mulo de entrada com uma intensidade m√≠nima. Uma vez atingido esse limiar, um impulso el√©trico se propaga ao longo do ax√¥nio, alcan√ßando o corpo celular e, subsequentemente, outros neur√¥nios.

De maneira an√°loga, o neur√¥nio artificial recebe sinais que passam por uma fun√ß√£o de ativa√ß√£o, gerando o sinal de sa√≠da do neur√¥nio. Uma caracter√≠stica importante √© que cada sinal de entrada pode ter uma relev√¢ncia diferente para o neur√¥nio, determinada pelo seu peso associado. Esse peso indica a contribui√ß√£o de cada sinal para a ativa√ß√£o do neur√¥nio.

As redes neurais se destacam por considerar as intera√ß√µes entre os seus elementos. Essa capacidade as torna particularmente eficazes no processamento de dados complexos como texto, v√≠deo e som. A arquitetura b√°sica envolve n√≥s de entrada que se conectam a n√≥s em uma ou mais camadas escondidas, culminando em um n√≥ de sa√≠da. A capacidade da rede de capturar intera√ß√µes complexas tende a aumentar com o n√∫mero de n√≥s presentes nas camadas escondidas. A conex√£o entre a camada de entrada e a camada escondida √© caracterizada pelos pesos associados a cada liga√ß√£o.

#### Redes Neurais Recorrentes (RNN)

As Redes Neurais Recorrentes (RNN) surgem como uma solu√ß√£o para o processamento de dados sequenciais, superando as limita√ß√µes das redes neurais feedforward tradicionais ao introduzirem o conceito de loops. Esses loops permitem que a informa√ß√£o seja persistida ao longo da sequ√™ncia.

**Funcionamento de uma RNN Padr√£o:**

Observe a seguinte representa√ß√£o visual do funcionamento de uma RNN padr√£o:

![RNN Padr√£o](https://miro.medium.com/v2/resize:fit:640/format:webp/0*6QxUX5KrH77f6Lm1.png)

As RNNs s√£o comumente utilizadas para lidar com dados sequenciais e s√©ries temporais. Podemos representar uma sequ√™ncia de dados como $x = (x^{(1)}, x^{(2)}, ..., x^{(t)})$, onde $t$ representa o instante de tempo, variando de um momento inicial 1 at√© um tempo $t$ (conceitualmente, uma linha do tempo).

**Arquitetura da RNN:**

A arquitetura de uma RNN pode ser visualizada na seguinte imagem (a RNN √© destacada na parte direita):

![Arquitetura RNN](https://miro.medium.com/v2/resize:fit:640/format:webp/1*JOkrQoJ3J3-451GzRcayRg.png)

Nessa representa√ß√£o, observamos um neur√¥nio (denotado por $h$) que recebe $x^{(t)}$ como entrada no instante de tempo $t$. Por exemplo, $x^{(t)}$ poderia ser uma palavra em uma frase na posi√ß√£o $t$.

$h^{(t)}$ representa o estado escondido (*Hidden State*), um conceito que ser√° detalhado posteriormente. O valor de $h^{(t)}$ √© calculado com base na entrada atual ($x^{(t)}$) e no estado escondido do instante de tempo anterior ($h^{(t-1)}$), de acordo com a seguinte f√≥rmula:

$$h^{(t)} = f(U x^{(t)} + W h^{(t-1)})$$

onde $f$ √© uma fun√ß√£o de transforma√ß√£o n√£o linear, como a tangente hiperb√≥lica (tanh) ou a fun√ß√£o ReLU.

A RNN possui conex√µes de entrada para a camada escondida parametrizadas por uma matriz de pesos $U$, conex√µes recorrentes entre as unidades escondidas parametrizadas por uma matriz de pesos $W$, e conex√µes da camada escondida para a sa√≠da parametrizadas por uma matriz de pesos $V$. Um aspecto fundamental das RNNs √© que todos esses pesos ($U$, $V$, e $W$) s√£o compartilhados ao longo do tempo.
Com certeza! Aqui est√° o texto formatado em Markdown, organizado e com melhor coes√£o:

```markdown
### Neur√¥nio Artificial e Fun√ß√£o de Ativa√ß√£o

A opera√ß√£o fundamental de um neur√¥nio artificial envolve uma **fun√ß√£o linear**, que pode ser expressa como:

$$(x_i \cdot W_{ij}) + bias$$

onde $x_i$ representa as entradas, $W_{ij}$ s√£o os pesos associados a essas entradas, e o *bias* √© um termo constante.

O resultado dessa fun√ß√£o linear √© ent√£o passado para uma **fun√ß√£o de ativa√ß√£o**. O papel crucial da fun√ß√£o de ativa√ß√£o √© determinar quais sinais ser√£o considerados relevantes e propagados para a pr√≥xima camada da rede neural. Ela introduz n√£o-linearidade ao modelo, permitindo que a rede aprenda rela√ß√µes complexas nos dados.

Uma fun√ß√£o de ativa√ß√£o amplamente utilizada √© a **ReLU (Rectified Linear Unit)**. Sua defini√ß√£o matem√°tica √© simples:

$$ReLU(x) = \begin{cases} x, & \text{se } x > 0 \\ 0, & \text{se } x \le 0 \end{cases}$$

Isso significa que a sa√≠da da ReLU √© o pr√≥prio valor de entrada se ele for positivo, e zero caso contr√°rio.

√â importante notar que o **tamanho do vetor de pesos** em um neur√¥nio artificial √© sempre igual ao tamanho do vetor de entrada. Isso garante que cada entrada tenha um peso correspondente que determine sua influ√™ncia na sa√≠da do neur√¥nio.

Em ess√™ncia, uma **rede neural** pode ser vista como um **grafo computacional**, onde os n√≥s representam as opera√ß√µes (neur√¥nios e fun√ß√µes de ativa√ß√£o) e as arestas representam o fluxo de dados.

A opera√ß√£o $(x_i \cdot W_{ij}) + bias$ √© um exemplo de uma **combina√ß√£o afim**.

### Prepara√ß√£o de Dados com PyTorch

O c√≥digo a seguir demonstra como criar um conjunto de dados alg√©brico personalizado e carregadores de dados utilizando a biblioteca PyTorch:

```python
import torch
from torch.utils.data import Dataset, DataLoader
import numpy.random as urand

class AlgebraicDataset(Dataset):
    """
    Cria um conjunto de dados de pares ordenados (x, f(x)).
    """
    def __init__(self, f, interval, nsamples):
        """
        Args:
            f (callable): A fun√ß√£o a ser aplicada aos valores de x.
            interval (list): O intervalo [inicio, fim] de onde os valores de x ser√£o amostrados.
            nsamples (int): O n√∫mero de amostras a serem geradas.
        """
        x = urand.uniform(interval[0], interval[1], nsamples)
        self.data = [(xi, f(xi)) for xi in x]

    def __len__(self):
        """
        Retorna o n√∫mero total de amostras no dataset.
        """
        return len(self.data)

    def __getitem__(self, idx):
        """
        Retorna o item (x, f(x)) no √≠ndice especificado.
        """
        return self.data[idx]

# Defini√ß√£o da fun√ß√£o linear, intervalo e n√∫mero de amostras
line = lambda x: 2 * x + 3
interval = [-10, 10]
train_nsamples = 1000
test_nsamples = 100

# Cria√ß√£o dos datasets de treinamento e teste
train_dataset = AlgebraicDataset(line, interval, train_nsamples)
test_dataset = AlgebraicDataset(line, interval, test_nsamples)

# Cria√ß√£o dos DataLoaders para itera√ß√£o eficiente sobre os datasets
train_dataloader = DataLoader(train_dataset, batch_size=train_nsamples, shuffle=True)
test_dataloader = DataLoader(test_dataset, batch_size=test_nsamples, shuffle=True)

# Verifica√ß√£o do dispositivo de treinamento (GPU se dispon√≠vel, CPU caso contr√°rio)
device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"Rodando na {device}")

# Supondo que LineNetwork() seja uma classe de modelo definida em outro lugar
# model = LineNetwork().to(device)
```

O `AlgebraicDataset` personalizado gera pares ordenados $(x, f(x))$ dentro de um intervalo especificado. O `DataLoader` do PyTorch facilita a leitura dos dados em batches, o que √© crucial para o treinamento eficiente de redes neurais. O par√¢metro `shuffle=True` garante que os dados sejam embaralhados em cada √©poca de treinamento, ajudando a evitar que o modelo aprenda a ordem dos dados.

### Treinamento de Redes Neurais

O treinamento de uma rede neural envolve ajustar seus **par√¢metros** (pesos e biases) para que o modelo possa mapear as entradas para as sa√≠das desejadas com a maior precis√£o poss√≠vel. A rede √© um **modelo param√©trico de uma fun√ß√£o**, e esses par√¢metros s√£o refinados durante o processo de aprendizagem.

Para que o aprendizado ocorra, a rede precisa de **feedback** para avaliar seu desempenho e ajustar seus par√¢metros na dire√ß√£o correta. Isso √© feito atrav√©s da **fun√ß√£o de perda** (*loss function*), que quantifica o erro entre as previs√µes do modelo e os valores reais. A fun√ß√£o de perda √© uma m√©trica que indica o qu√£o perto ou qu√£o longe as previs√µes est√£o da verdade.

Um exemplo comum de fun√ß√£o de perda √© o **erro quadr√°tico m√©dio (MSE - Mean Squared Error)**, calculado pela m√©dia da soma dos quadrados das diferen√ßas entre as previs√µes e os valores reais. A raiz quadrada do MSE √© o **RMSE (Root Mean Squared Error)**.

```python
import torch.nn as nn
import torch.optim as optim

# Fun√ß√£o de perda: Erro Quadr√°tico M√©dio
lossfunc = nn.MSELoss()

# Otimizador: Gradiente Descendente Estoc√°stico (SGD)
# optimizer = optim.SGD(model.parameters(), lr=learning_rate)
```

Para minimizar a fun√ß√£o de perda e melhorar o desempenho do modelo, utilizamos algoritmos de otimiza√ß√£o, como o **Gradiente Descendente**. O Gradiente Descendente √© um m√©todo iterativo que ajusta os par√¢metros do modelo na dire√ß√£o do negativo do gradiente da fun√ß√£o de perda em rela√ß√£o a esses par√¢metros.

A vers√£o **Estoc√°stica (SGD)** do Gradiente Descendente calcula o gradiente usando apenas um subconjunto aleat√≥rio dos dados (um batch) em cada itera√ß√£o. Essa abordagem introduz aleatoriedade, o que pode ajudar a escapar de m√≠nimos locais na fun√ß√£o de perda. A consequ√™ncia de usar apenas partes dos dados em cada atualiza√ß√£o √© o que torna o m√©todo estoc√°stico.

√â importante notar que o Gradiente Descendente em sua forma b√°sica pode n√£o identificar se o modelo atingiu um m√≠nimo global na fun√ß√£o de perda.

O **tamanho do passo** dado na dire√ß√£o do gradiente √© controlado pela **taxa de aprendizado (*learning rate* - `lr`)**, um hiperpar√¢metro crucial que precisa ser ajustado cuidadosamente.

```python
def test(model, dataloader, lossfunc):
    """
    Avalia o desempenho do modelo no conjunto de dados fornecido.
    """
    model.eval()  # Coloca o modelo em modo de avalia√ß√£o
    cumloss = 0.0
    with torch.no_grad():  # Desativa o c√°lculo de gradientes durante a avalia√ß√£o
        for X, y in dataloader:
            X = X.unsqueeze(1).float().to(device)
            y = y.unsqueeze(1).float().to(device)
            pred = model(X)
            loss = lossfunc(pred, y)
            cumloss += loss.item()
    return cumloss / len(dataloader)

# O tamanho do passo (learning rate) do gradiente descendente pode ser modificado
```

### Otimiza√ß√£o e Conceitos Avan√ßados

Al√©m do Gradiente Descendente, existem outros **algoritmos de otimiza√ß√£o**, como algoritmos gen√©ticos e otimiza√ß√£o por enxame de part√≠culas (PSO), que podem ser usados para treinar redes neurais e otimizar seus **hiperpar√¢metros** (par√¢metros que n√£o s√£o aprendidos durante o treinamento, como a taxa de aprendizado e a arquitetura da rede).

Outras t√©cnicas e conceitos importantes incluem:

* **Implementa√ß√£o manual de embaralhamento (shuffle) em Python:** Permite um controle mais granular sobre o processo de aleatoriza√ß√£o dos dados.
* **Implementa√ß√£o manual de Min-Max Scaling em Python:** Uma t√©cnica de normaliza√ß√£o que escala os dados para um intervalo espec√≠fico (geralmente \[0, 1\] ou \[-1, 1\]). √â importante calcular os valores m√≠nimo e m√°ximo apenas no conjunto de treinamento para evitar o vazamento de informa√ß√µes do conjunto de teste.
* **ELM (Extreme Learning Machine):** Uma arquitetura de rede neural com camadas ocultas de ativa√ß√£o aleat√≥ria e pesos de sa√≠da determinados analiticamente.
* **√Ålgebra Matricial e Implementa√ß√£o:** A compreens√£o da √°lgebra matricial √© fundamental para implementar e otimizar opera√ß√µes em redes neurais.
* **Processo de Desnormaliza√ß√£o:** A revers√£o de t√©cnicas de normaliza√ß√£o para interpretar as previs√µes do modelo na escala original dos dados.

### Conceitos B√°sicos de Processos Estoc√°sticos e S√©ries Temporais

* **Processos Estoc√°sticos e S√©ries Temporais:** Um processo estoc√°stico √© uma sequ√™ncia de vari√°veis aleat√≥rias indexadas por tempo ou espa√ßo. Uma s√©rie temporal √© uma sequ√™ncia de dados coletados em pontos de tempo sucessivos.
* **Estacionariedade:** Uma propriedade de um processo estoc√°stico onde suas propriedades estat√≠sticas (m√©dia, vari√¢ncia, autocovari√¢ncia) n√£o mudam ao longo do tempo.
* **Fun√ß√£o de Autocovari√¢ncia e Espectro:** A fun√ß√£o de autocovari√¢ncia mede a correla√ß√£o entre os valores de um processo em diferentes pontos no tempo. O espectro (ou densidade espectral de pot√™ncia) descreve a distribui√ß√£o da pot√™ncia do sinal em diferentes frequ√™ncias.

### Processos ARMA Estacion√°rios

* **Modelos Autoregressivos (AR):** Modelos que preveem o valor atual de uma s√©rie temporal com base em seus valores passados.
* **Modelos de M√©dias M√≥veis (MA):** Modelos que preveem o valor atual com base em erros de previs√£o passados.
* **Modelos ARMA (Autoregressive Moving Average):** Combinam componentes autoregressivos e de m√©dias m√≥veis.
* **Modelos ARIMA (Autoregressive Integrated Moving Average):** Estendem os modelos ARMA para lidar com s√©ries temporais n√£o estacion√°rias atrav√©s da diferencia√ß√£o.
* **Modelo Linear Geral e Modelos Harm√¥nicos:** Outras abordagens para modelar s√©ries temporais.

### An√°lise Espectral

* **S√©ries de Fourier:** Uma representa√ß√£o de uma fun√ß√£o peri√≥dica como uma soma de senos e cossenos.
* **An√°lise de Fun√ß√µes Peri√≥dicas e N√£o Peri√≥dicas:** A an√°lise de Fourier pode ser estendida para fun√ß√µes n√£o peri√≥dicas atrav√©s da Transformada de Fourier.
* **Representa√ß√£o Espectral de Processos Estacion√°rios:** O espectro de um processo estacion√°rio fornece informa√ß√µes sobre suas componentes de frequ√™ncia.
* **Espectro Misto e Filtros Lineares:** Processos podem ter um espectro com componentes discretas e cont√≠nuas. Filtros lineares podem ser aplicados para modificar o espectro de um sinal.

### Estima√ß√£o no Dom√≠nio do Tempo

* **Estima√ß√£o da M√©dia e da Fun√ß√£o de Autocovari√¢ncia:** M√©todos para estimar essas propriedades estat√≠sticas a partir de dados de s√©ries temporais.
* **Identifica√ß√£o, Estima√ß√£o e Previs√£o de Par√¢metros de Modelos ARIMA:** T√©cnicas para determinar a ordem de um modelo ARIMA e estimar seus par√¢metros, bem como usar o modelo para fazer previs√µes futuras.

### Estima√ß√£o no Dom√≠nio da Frequ√™ncia

* **A Transformada de Fourier Finita e o Periodograma:** Ferramentas para estimar o espectro de uma s√©rie temporal finita.
* **Estimadores Suavizados:** M√©todos para reduzir a vari√¢ncia nas estimativas espectrais obtidas do periodograma.
```
---
Divis√£o dos Dados (Train/Test/Validation Split): Separar os dados hist√≥ricos em conjuntos para treinamento do modelo, teste e, possivelmente, valida√ß√£o. √â crucial que essa divis√£o respeite a ordem temporal, ou seja, os dados de teste devem ser posteriores aos dados de treino

Estat√≠sticas de Janela (Rolling/Expanding Window Statistics): C√°lculos como m√©dia m√≥vel, desvio padr√£o m√≥vel, etc., sobre per√≠odos recentes

Lags da pr√≥pria s√©rie: Valores passados da s√©rie que servem como preditores. A an√°lise de ACF/PACF pode ajudar a identificar lags relevantes

Sele√ß√£o e Defini√ß√£o do(s) Modelo(s): Escolher o(s) algoritmo(s) de previs√£o a serem utilizados, como ARIMA, modelos de Suaviza√ß√£o Exponencial (como Holt-Winters), modelos de Machine Learning (como Regress√£o Linear, Random Forest, SVM) ou Redes Neurais (como LSTMs), ou uma combina√ß√£o/abordagem h√≠brida

Treinamento do(s) Modelo(s): Ajustar o(s) modelo(s) selecionado(s) aos dados de treinamento. Isso pode envolver a otimiza√ß√£o de hiperpar√¢metros

Treinamento do(s) Modelo(s): Ajustar o(s) modelo(s) selecionado(s) aos dados de treinamento. Isso pode envolver a otimiza√ß√£o de hiperpar√¢metros

Previs√£o (Forecasting): Utilizar o(s) modelo(s) treinado(s) para gerar previs√µes para per√≠odos futuros (no conjunto de teste ou valida√ß√£o)

Desnormaliza√ß√£o / Back-transformation (se aplic√°vel): Se a normaliza√ß√£o (como MinMax) foi aplicada √†s vari√°veis de entrada do modelo e/ou se a previs√£o de um componente (como o res√≠duo) foi feita em escala normalizada, a previs√£o gerada pelo modelo deve ser DESNORMALIZADA (transformada de volta) para a escala original da s√©rie temporal. Este passo √© fundamental para que a previs√£o final tenha a magnitude e unidades corretas [Como discutimos anteriormente; 470 menciona o processo de desnormaliza√ß√£o].

Combina√ß√£o / Recomposi√ß√£o (se aplic√°vel): Em abordagens que envolvem a decomposi√ß√£o da s√©rie ou o uso de m√∫ltiplos modelos para prever diferentes componentes (por exemplo, prever a sazonalidade separadamente ou modelar os res√≠duos de um primeiro modelo com um segundo modelo), as previs√µes dos componentes individuais (ap√≥s a desnormaliza√ß√£o, se necess√°rio) precisam ser combinadas para gerar a previs√£o final da s√©rie original [Como discutimos anteriormente; 140 descreve a combina√ß√£o de previs√µes da componente sazonal e da s√©rie ajustada; 461 menciona combina√ß√£o de modelos].

Avalia√ß√£o do Desempenho: Comparar as previs√µes finais (agora na escala original) com os valores reais no conjunto de teste/valida√ß√£o usando m√©tricas de avalia√ß√£o apropriadas (como RMSE, MAE, MAPE, etc.)

Valida√ß√£o dos Res√≠duos do Modelo Final: Ap√≥s o modelo ser ajustado e as previs√µes geradas, √© importante analisar os res√≠duos (a diferen√ßa entre os valores reais e as previs√µes no conjunto de treino/valida√ß√£o) para garantir que eles se assemelham a um ru√≠do branco (ou seja, n√£o h√° mais padr√µes temporais n√£o explicados pelo modelo)
---
# üìö Reposit√≥rio de Links e Materiais

## üéì Aulas e Google Classroom

- [Google Classroom - Aula](https://classroom.google.com/c/NzQ4MzgyNzkxOTgz/m/Njk5MzYzNzY5OTEx/details)
- [YouTube - Aula LSTM Parte 1](https://www.youtube.com/watch?v=FbxTVRfQFuI&list=PLEiEAq2VkUUIYQ-mMRAGilfOKyWKpHSip&index=4)
- [YouTube - Aula LSTM Parte 2](https://www.youtube.com/watch?v=lWkFhVq9-nc&t=214s)

## üíª Google Colab e Notebooks

- [Colab - Exerc√≠cios de LSTM](https://colab.research.google.com/drive/1Vs0Po5UXSaaGwQ7aFXIXmzJlO2ltv9RV?usp=sharing&authuser=0)

## üß† Redes Neurais Recorrentes (RNN, LSTM, GRU)

- [Kaggle - RNN Tutorial por kcsener](https://www.kaggle.com/code/kcsener/8-recurrent-neural-network-rnn-tutorial)
- [Kaggle - LSTM e GRU Tutorial por thebrownviking20](https://www.kaggle.com/code/thebrownviking20/intro-to-recurrent-neural-networks-lstm-gru)
- [Kaggle - Decomposi√ß√£o de S√©ries Temporais](https://www.kaggle.com/code/chanakyavivekkapoor/decomposing-time-series-data)

## üìÅ GitHub e Artigos T√©cnicos

- [GitHub - BRITS](https://github.com/caow13/BRITS)
- [NeurIPS Paper 2018 - BRITS](https://papers.nips.cc/paper_files/paper/2018/file/734e6bfcd358e25ac1db0a4241b95651-Paper.pdf)
- [PapersWithCode - BRITS](https://paperswithcode.com/paper/brits-bidirectional-recurrent-imputation-for)

---

## üî∏ Hybrid Mode (Modelos H√≠bridos)

- [Medium - Classificadores H√≠bridos em S√©ries Temporais](https://medium.com/@preeti.rana.ai/hybrid-classifiers-time-series-forecasting-88594988cc44)
- [Medium - Prophet + LSTM](https://subashpalvel.medium.com/time-series-forecasting-with-prophet-and-lstm-hybrid-mode-75f5295605e5)
- [YouTube - Tutorial H√≠brido LSTM + Prophet](https://www.youtube.com/watch?v=E2_IhBKxBxM)

---

## ü§ñ Otimiza√ß√£o de Modelos

### ‚öôÔ∏è Algoritmos Evolutivos (PSO)

- [PSO explica√ß√£o simples com Python - Nathan.fun](https://nathan.fun/posts/2016-08-17/simple-particle-swarm-optimization-with-python/)
- [PSO em Python - GeeksforGeeks](https://www.geeksforgeeks.org/implementation-of-particle-swarm-optimization/)
- [PSO explica√ß√£o pr√°tica - Medium](https://medium.com/data-science/what-the-hell-is-particle-swarm-optimization-pso-simplest-explanation-in-python-be296fc3b1ab)
- [PSO c√≥digo e visualiza√ß√£o - Towards Data Science](https://towardsdatascience.com/swarm-intelligence-coding-and-visualising-particle-swarm-optimisation-in-python-253e1bd00772/)
- [PSO implementado em Python - Medium](https://induraj2020.medium.com/implementing-particle-swarm-optimization-in-python-c59278bc5846)

---

## üîé Artigos Cient√≠ficos e Livros

### üìò Livros Base

- *An√°lise de S√©ries Temporais* ‚Äì Morettin & Toloi  
- *Econometria B√°sica* ‚Äì Damodar Gujarati  
- *Elementos de Estat√≠stica Computacional* ‚Äì Frery & Cribari Neto  
- *Machine Learning* ‚Äì Tom Mitchell ([PDF](https://www.cs.cmu.edu/~tom/files/MachineLearningTomMitchell.pdf))

### üìÑ Artigos do Prof. Fausto

- [IEEExplore 2014 - Previs√£o com H√≠brido PSO + SVM](https://ieeexplore.ieee.org/abstract/document/6974534)
- [Neurocomputing 2015 - H√≠brido com Wavelet](https://www.sciencedirect.com/science/article/abs/pii/S0925231215016057)
- [Knowledge-Based Systems 2019 - ELM Otimizado](https://www.sciencedirect.com/science/article/abs/pii/S0950705119301327)
- [IEEE 2021 - Forecast com PSO + Deep Learning](https://ieeexplore.ieee.org/abstract/document/9340584)

### üìë Outros Artigos

- [Forecasting with artificial neural networks: The state of the art (OSC Open)](https://www.oscogen.ethz.ch/members/literature_restricted%20access/ann_for_001.pdf)

---

## üî¨ An√°lise e Detec√ß√£o de Anomalias em S√©ries Temporais

- [arXiv - Deep Learning for Time Series Anomaly Detection: A Survey](https://arxiv.org/abs/1905.13628)
- [ScienceDirect - Anomaly detection in time series](https://www.sciencedirect.com/science/article/pii/S0378437121004076)
- [IEEE - Review on Outlier/Anomaly Detection](https://ieeexplore.ieee.org/document/4626688)
- [ACM - DL for Anomaly Detection in Time-Series](https://dl.acm.org/doi/full/10.1145/3691338)
- [ACM - Microsoft Time-Series Anomaly Detection Service](https://dl.acm.org/doi/abs/10.14778/3538598.3538602)
- [ACM - Anomaly Detection on Time Series](https://dl.acm.org/doi/abs/10.1145/3292500.3330680)
- [IEEE - CNN + Transfer Learning for Time Series Anomaly Detection](https://ieeexplore.ieee.org/abstract/document/8926446)

---

## üß† Previs√£o H√≠brida e Otimiza√ß√£o

- [Hybrid Time Series Forecasting Methods for Travel Time Prediction (ScienceDirect)](https://www.sciencedirect.com/science/article/pii/S111001682100613X)
- [Understanding Memetic Algorithms (IndiaAI)](https://indiaai.gov.in/article/understanding-memetic-algorithm)
- [Computationally Efficient Hybrid ARIMA-SVR Model (IEEE)](https://ieeexplore.ieee.org/abstract/document/9523565)
- [Memetic Algorithm for Pattern Recognition (CBRN)](https://sbic.org.br/eventos/cbrn_2007/50100078-2/)

---

## üßÆ Modelos ARIMA e SARIMA

- [ARIMA do zero em Python - Medium](https://medium.com/analytics-vidhya/arima-model-from-scratch-in-python-489e961603ce)
- [Como o ARIMA funciona - Burak Ayy](https://burakayy.com/blog/how-arima-works)
- [Treinamento SARIMA passo a passo - MLPills](https://mlpills.dev/time-series/how-to-train-a-sarima-model-step-by-step/)
- [Compara√ß√£o ARIMA, SARIMA, SARIMAX - Towards Data Science](https://towardsdatascience.com/time-series-forecasting-with-arima-sarima-and-sarimax-ee61099e78f6/)
- [Forecasting com SARIMA - Medium](https://medium.com/@ozdogar/time-series-forecasting-using-sarima-python-8db28f1d8cfc)
- [Estimando coeficientes ARIMA/SARIMA com GA - Medium](https://medium.com/@mouse3mic3/estimating-arima-and-sarima-coefficients-using-genetic-algorithm-03f24ab66589)
- [ARIMA from scratch part 1 - Medium](https://medium.com/@learn-simplified/build-arima-model-from-scratch-part-1-b72b73ba230f)
- [ARIMA Tutorial - Datacamp](https://www.datacamp.com/tutorial/arima)
- [ARIMA from scratch - Kaggle](https://www.kaggle.com/code/phunghieu/arima-from-scratch)

---

## üîÅ RNN, LSTM e S√©ries Temporais com Deep Learning

- [LSTM com Tensorflow/Keras - Curiousily](https://curiousily.com/posts/time-series-forecasting-with-lstms-using-tensorflow-2-and-keras-in-python/)
- [LSTM para COVID-19 - Curiousily](https://curiousily.com/posts/time-series-forecasting-with-lstm-for-daily-coronavirus-cases/)
- [Previs√£o de Demanda com LSTM - Curiousily](https://curiousily.com/posts/demand-prediction-with-lstms-using-tensorflow-2-and-keras-in-python/)
- [LSTM no YouTube (tutorial)](https://www.youtube.com/watch?v=c0k-YLQGKjY&t=1379s)
- [Kaggle - LSTM em S√©ries Temporais (Laborat√≥rio)](https://www.kaggle.com/code/paulorzp/laborat-rio-12b-usando-lstm-em-s-ries-temporais)
- [Vantagens e Desvantagens do LSTM - Medium](https://medium.com/@prudhviraju.srivatsavaya/lstm-implementation-advantages-and-diadvantages)
